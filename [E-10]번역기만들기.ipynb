{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b779d36",
   "metadata": {},
   "source": [
    "# 루브릭\n",
    "|평가문항|상세기준|self check|\n",
    "|:-------|:-------|:-----------------------|\n",
    "|1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.|구두점, 대소문자, 띄어쓰기 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.|OK|\n",
    "|2. seq2seq 기반의 번역기 모델이 정상적으로 구동된다.|seq2seq 모델 훈련결과를 그래프로 출력해보고, validation loss그래프가 우하향하는 경향성을 보이며 학습이 진행됨이 확인되었다.|OK|\n",
    "|3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|테스트용 디코더 모델이 정상적으로 만들어졌으며, input(영어)와 output(프랑스어) 모두 한글로 번역해서 결과를 출력해보았고, 둘의 내용이 유사함을 확인하였다.|OK|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2ee37",
   "metadata": {},
   "source": [
    "### dataset\n",
    "1. 프랑스어와 영어의 병렬 코퍼스\n",
    "   * [fra-eng.zip](https://www.manythings.org/anki/)\n",
    "   * Random 33,000 개의 sample \n",
    "\n",
    "### sequence-to-sequence model 설계\n",
    "1. RNN (LTSM)\n",
    "   * Encorder\n",
    "   * Decorder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660072d",
   "metadata": {},
   "source": [
    "## 문제 정의  \n",
    "\n",
    "> Word-level Machine Translation 만들기  \n",
    "> RNN을 사용한 word-level seq2seq로 기계 번역기를 만들기   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf49d6",
   "metadata": {},
   "source": [
    "## 라이브러리 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c9887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3161329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec939373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126320</th>\n",
       "      <td>The tower can be seen from here.</td>\n",
       "      <td>La tour peut être vue d'ici.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142200</th>\n",
       "      <td>It may be that he is not a bad man.</td>\n",
       "      <td>Il se peut qu'il ne soit pas un homme mauvais.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143118</th>\n",
       "      <td>The problem is being discussed now.</td>\n",
       "      <td>On est en train de parler du problème en ce mo...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85098</th>\n",
       "      <td>You're very sophisticated.</td>\n",
       "      <td>Vous êtes très raffiné.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176390</th>\n",
       "      <td>Tom asked Mary who had given her the picture.</td>\n",
       "      <td>Tom a demandé à Mary qui lui a donné la photo.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  eng  \\\n",
       "126320               The tower can be seen from here.   \n",
       "142200            It may be that he is not a bad man.   \n",
       "143118            The problem is being discussed now.   \n",
       "85098                      You're very sophisticated.   \n",
       "176390  Tom asked Mary who had given her the picture.   \n",
       "\n",
       "                                                      fra  \\\n",
       "126320                       La tour peut être vue d'ici.   \n",
       "142200     Il se peut qu'il ne soit pas un homme mauvais.   \n",
       "143118  On est en train de parler du problème en ce mo...   \n",
       "85098                             Vous êtes très raffiné.   \n",
       "176390     Tom a demandé à Mary qui lui a donné la photo.   \n",
       "\n",
       "                                                       cc  \n",
       "126320  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "142200  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "143118  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "85098   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "176390  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng         fra                                                 cc\n",
       "0  Go.        Va !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1  Go.     Marche.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "2  Go.  En route !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "3  Go.     Bouge !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "4  Hi.     Salut !  CC-BY 2.0 (France) Attribution: tatoeba.org #5..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = './data/fra.txt'\n",
    "\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17a695",
   "metadata": {},
   "source": [
    "* 33000개의 random sample 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec67958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84023</th>\n",
       "      <td>What did you get hit with?</td>\n",
       "      <td>Avec quoi avez-vous été frappée ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179408</th>\n",
       "      <td>I'm thinking of visiting you one of these days.</td>\n",
       "      <td>Je pense à te rendre visite un de ces quatre.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127921</th>\n",
       "      <td>You should've gotten up earlier.</td>\n",
       "      <td>Vous auriez dû vous lever plus tôt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22856</th>\n",
       "      <td>Did you eat lunch?</td>\n",
       "      <td>As-tu pris un déjeuner ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105898</th>\n",
       "      <td>This hat is too small for me.</td>\n",
       "      <td>Ce chapeau est trop petit pour moi.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    eng  \\\n",
       "84023                        What did you get hit with?   \n",
       "179408  I'm thinking of visiting you one of these days.   \n",
       "127921                 You should've gotten up earlier.   \n",
       "22856                                Did you eat lunch?   \n",
       "105898                    This hat is too small for me.   \n",
       "\n",
       "                                                  fra  \n",
       "84023               Avec quoi avez-vous été frappée ?  \n",
       "179408  Je pense à te rendre visite un de ces quatre.  \n",
       "127921            Vous auriez dû vous lever plus tôt.  \n",
       "22856                        As-tu pris un déjeuner ?  \n",
       "105898            Ce chapeau est trop petit pour moi.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sklearn.utils.shuffle(lines)\n",
    "lines = lines[['eng', 'fra']][:33000] # 33000 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2525db81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74256</th>\n",
       "      <td>Take this to your mother.</td>\n",
       "      <td>Apporte ceci à ta mère.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5645</th>\n",
       "      <td>Make a guess.</td>\n",
       "      <td>Hasarde une hypothèse !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193849</th>\n",
       "      <td>Wimbledon has eighteen grass courts, including...</td>\n",
       "      <td>Wimbledon a dix-huit courts sur gazon, dont le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34765</th>\n",
       "      <td>Do you want a salad?</td>\n",
       "      <td>Tu veux une salade ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148582</th>\n",
       "      <td>Unfortunately, I missed all the fun.</td>\n",
       "      <td>Malheureusement, j'ai loupé tout l'amusement.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "74256                           Take this to your mother.   \n",
       "5645                                        Make a guess.   \n",
       "193849  Wimbledon has eighteen grass courts, including...   \n",
       "34765                                Do you want a salad?   \n",
       "148582               Unfortunately, I missed all the fun.   \n",
       "\n",
       "                                                      fra  \n",
       "74256                             Apporte ceci à ta mère.  \n",
       "5645                              Hasarde une hypothèse !  \n",
       "193849  Wimbledon a dix-huit courts sur gazon, dont le...  \n",
       "34765                                Tu veux une salade ?  \n",
       "148582      Malheureusement, j'ai loupé tout l'amusement.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#상위 5개 sample\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5749335",
   "metadata": {},
   "source": [
    "## Step 1. 정제, 정규화, 전처리 (영어, 프랑스어 모두!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9596da",
   "metadata": {},
   "source": [
    "1. 구두점(Punctuation)을 단어와 분리\n",
    "   * !나 ? 또는 온점과 같은 특수문자들을 구두점(punctuation)\n",
    "2. 소문자로 바꾸기\n",
    "3. 띄어쓰기 단위로 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade9ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouvre moiv ! .\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    #sentence = '<sos> ' + sentence + ' <eos>' # 6\n",
    "\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"Ouvre-moiv !.    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb52e5",
   "metadata": {},
   "source": [
    "* englsih : 정제된 문장 eng_corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9603b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['take this to your mother .',\n",
       " 'make a guess .',\n",
       " 'wimbledon has eighteen grass courts , including the center court .',\n",
       " 'do you want a salad ?',\n",
       " 'unfortunately , i missed all the fun .',\n",
       " 'are you following me ?',\n",
       " 'we want to see the king .',\n",
       " 'we ll never catch tom .',\n",
       " 'how will you be spending christmas ?',\n",
       " 'a beautiful sunset , isn t it ?',\n",
       " 'it s a girl thing .',\n",
       " 'you should get married again .',\n",
       " 'almost everyone was dead .',\n",
       " 'it is no good to you .',\n",
       " 'she invited us to her birthday party .',\n",
       " 'is this book boring ?',\n",
       " 'i bought it .',\n",
       " 'i understood what you meant .',\n",
       " 'slavery is a crime against humanity .',\n",
       " 'i m not like the rest of these guys .']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "eng_corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in lines['eng']:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: \n",
    "        print(sentence)\n",
    "        continue\n",
    "    \n",
    "    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주세요\n",
    "    eng_preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    eng_corpus.append(eng_preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "len(eng_corpus)\n",
    "eng_corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d9115",
   "metadata": {},
   "source": [
    "## Step 2. 디코더의 문장에 시작 토큰과 종료 토큰을 넣어주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9f58c",
   "metadata": {},
   "source": [
    "* franch : 정제된 문장 fra_corpus \n",
    "* 입력 시퀀스 : ['\\<sos\\>', 'courez', '!']\n",
    "* 레이블 시퀀스 : ['courez', '!', '\\<eos\\>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf44309a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['<sos> apporte ceci ta m re . <eos>',\n",
       " '<sos> hasarde une hypoth se ! <eos>',\n",
       " '<sos> wimbledon a dix huit courts sur gazon , dont le court central . <eos>',\n",
       " '<sos> tu veux une salade ? <eos>',\n",
       " '<sos> malheureusement , j ai loup tout l amusement . <eos>',\n",
       " '<sos> est ce que vous me suivez ? <eos>',\n",
       " '<sos> nous voulons voir le roi . <eos>',\n",
       " '<sos> nous n attraperons jamais tom . <eos>',\n",
       " '<sos> comment passeras tu no l ? <eos>',\n",
       " '<sos> joli coucher de soleil , n est ce pas ? <eos>',\n",
       " '<sos> c est un truc de filles . <eos>',\n",
       " '<sos> vous devriez vous remarier . <eos>',\n",
       " '<sos> presque tout le monde tait mort . <eos>',\n",
       " '<sos> a ne te fait pas du bien . <eos>',\n",
       " '<sos> elle nous invita sa f te d anniversaire . <eos>',\n",
       " '<sos> est ce que ce livre est ennuyant ? <eos>',\n",
       " '<sos> je l ai achet . <eos>',\n",
       " '<sos> j ai compris ce que vous vouliez dire . <eos>',\n",
       " '<sos> l esclavage est un crime contre l humanit . <eos>',\n",
       " '<sos> je ne suis pas comme le reste de ces types . <eos>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "fra_corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in lines['fra']:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "       \n",
    "    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주세요\n",
    "    fra_preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    fra_preprocessed_sentence = '<sos> ' + fra_preprocessed_sentence + ' <eos>' # 6\n",
    "    fra_corpus.append(fra_preprocessed_sentence)\n",
    "            \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "len(fra_corpus)\n",
    "fra_corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f309f7",
   "metadata": {},
   "source": [
    "## Step 3. 케라스의 토크나이저로 텍스트를 숫자로 바꿔보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6534ffc",
   "metadata": {},
   "source": [
    "* tf.keras.preprocessing.text.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9566810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[98, 17, 4, 28, 207, 1],\n",
       " [109, 7, 406, 1],\n",
       " [4713, 75, 2891, 1644, 3567, 23, 4714, 5, 1362, 2171, 1],\n",
       " [15, 3, 34, 7, 1530, 6],\n",
       " [1363, 23, 2, 609, 45, 5, 327, 1],\n",
       " [31, 3, 1281, 20, 6],\n",
       " [22, 34, 4, 82, 5, 1364, 1],\n",
       " [22, 49, 101, 491, 10, 1],\n",
       " [43, 65, 3, 33, 1105, 185, 6],\n",
       " [7, 318, 2892, 23, 129, 8, 12, 6]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(filters=' ')   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(eng_corpus)               # eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(eng_corpus)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3568acfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '.',\n",
       " 2: 'i',\n",
       " 3: 'you',\n",
       " 4: 'to',\n",
       " 5: 'the',\n",
       " 6: '?',\n",
       " 7: 'a',\n",
       " 8: 't',\n",
       " 9: 'is',\n",
       " 10: 'tom',\n",
       " 11: 'that',\n",
       " 12: 'it',\n",
       " 13: 's',\n",
       " 14: 'he',\n",
       " 15: 'do',\n",
       " 16: 'of',\n",
       " 17: 'this',\n",
       " 18: 'in',\n",
       " 19: 'don',\n",
       " 20: 'me',\n",
       " 21: 'have',\n",
       " 22: 'we',\n",
       " 23: ',',\n",
       " 24: 'was',\n",
       " 25: 'what',\n",
       " 26: 'my',\n",
       " 27: 'can',\n",
       " 28: 'your',\n",
       " 29: 'm',\n",
       " 30: 'for',\n",
       " 31: 'are',\n",
       " 32: 're',\n",
       " 33: 'be',\n",
       " 34: 'want',\n",
       " 35: 'she',\n",
       " 36: 'not',\n",
       " 37: 'like',\n",
       " 38: 'on',\n",
       " 39: 'know',\n",
       " 40: 'with',\n",
       " 41: 'they',\n",
       " 42: 'did',\n",
       " 43: 'how',\n",
       " 44: 'go',\n",
       " 45: 'all',\n",
       " 46: 'his',\n",
       " 47: 'at',\n",
       " 48: 'think',\n",
       " 49: 'll',\n",
       " 50: 'and',\n",
       " 51: 've',\n",
       " 52: 'there',\n",
       " 53: 'about',\n",
       " 54: 'here',\n",
       " 55: 'him',\n",
       " 56: 'time',\n",
       " 57: 'get',\n",
       " 58: 'very',\n",
       " 59: 'didn',\n",
       " 60: 'were',\n",
       " 61: 'one',\n",
       " 62: 'no',\n",
       " 63: 'out',\n",
       " 64: 'going',\n",
       " 65: 'will',\n",
       " 66: 'up',\n",
       " 67: 'if',\n",
       " 68: 'had',\n",
       " 69: 'why',\n",
       " 70: 'her',\n",
       " 71: 'd',\n",
       " 72: 'as',\n",
       " 73: 'just',\n",
       " 74: 'who',\n",
       " 75: 'has',\n",
       " 76: 'so',\n",
       " 77: 'would',\n",
       " 78: 'need',\n",
       " 79: 'let',\n",
       " 80: 'mary',\n",
       " 81: 'come',\n",
       " 82: 'see',\n",
       " 83: 'when',\n",
       " 84: 'from',\n",
       " 85: 'good',\n",
       " 86: 'help',\n",
       " 87: 'should',\n",
       " 88: 'tell',\n",
       " 89: 'really',\n",
       " 90: 'us',\n",
       " 91: 'an',\n",
       " 92: 'where',\n",
       " 93: 'could',\n",
       " 94: 'please',\n",
       " 95: 'by',\n",
       " 96: 'been',\n",
       " 97: 'more',\n",
       " 98: 'take',\n",
       " 99: 'something',\n",
       " 100: 'got',\n",
       " 101: 'never',\n",
       " 102: 'now',\n",
       " 103: 'too',\n",
       " 104: 'much',\n",
       " 105: '!',\n",
       " 106: 'work',\n",
       " 107: 'some',\n",
       " 108: 'home',\n",
       " 109: 'make',\n",
       " 110: 'thought',\n",
       " 111: 'last',\n",
       " 112: 'right',\n",
       " 113: 'any',\n",
       " 114: 'than',\n",
       " 115: 'but',\n",
       " 116: 'back',\n",
       " 117: 'sure',\n",
       " 118: 'doesn',\n",
       " 119: 'people',\n",
       " 120: 'money',\n",
       " 121: 'give',\n",
       " 122: 'anything',\n",
       " 123: 'still',\n",
       " 124: 'french',\n",
       " 125: 'look',\n",
       " 126: 'many',\n",
       " 127: 'lot',\n",
       " 128: 'car',\n",
       " 129: 'isn',\n",
       " 130: 'say',\n",
       " 131: 'am',\n",
       " 132: 'day',\n",
       " 133: 'way',\n",
       " 134: 'told',\n",
       " 135: 'talk',\n",
       " 136: 'made',\n",
       " 137: 'only',\n",
       " 138: 'house',\n",
       " 139: 'won',\n",
       " 140: 'again',\n",
       " 141: 'eat',\n",
       " 142: 'must',\n",
       " 143: 'said',\n",
       " 144: 'our',\n",
       " 145: 'love',\n",
       " 146: 'believe',\n",
       " 147: 'today',\n",
       " 148: 'went',\n",
       " 149: 'doing',\n",
       " 150: 'does',\n",
       " 151: 'off',\n",
       " 152: 'long',\n",
       " 153: 'happy',\n",
       " 154: 'stay',\n",
       " 155: 'always',\n",
       " 156: 'better',\n",
       " 157: 'them',\n",
       " 158: 'feel',\n",
       " 159: 'night',\n",
       " 160: 'these',\n",
       " 161: 'new',\n",
       " 162: 'little',\n",
       " 163: 'put',\n",
       " 164: 'room',\n",
       " 165: 'leave',\n",
       " 166: 'book',\n",
       " 167: 'tomorrow',\n",
       " 168: 'down',\n",
       " 169: 'old',\n",
       " 170: 'two',\n",
       " 171: 'yesterday',\n",
       " 172: 'before',\n",
       " 173: 'school',\n",
       " 174: 'everything',\n",
       " 175: 'job',\n",
       " 176: 'left',\n",
       " 177: 'try',\n",
       " 178: 'stop',\n",
       " 179: 'over',\n",
       " 180: 'may',\n",
       " 181: 'well',\n",
       " 182: 'speak',\n",
       " 183: 'father',\n",
       " 184: 'buy',\n",
       " 185: 'christmas',\n",
       " 186: 'enough',\n",
       " 187: 'asked',\n",
       " 188: 'or',\n",
       " 189: 'live',\n",
       " 190: 'every',\n",
       " 191: 'happened',\n",
       " 192: 'everyone',\n",
       " 193: 'three',\n",
       " 194: 'understand',\n",
       " 195: 'friends',\n",
       " 196: 'done',\n",
       " 197: 'into',\n",
       " 198: 'away',\n",
       " 199: 'things',\n",
       " 200: 'keep',\n",
       " 201: 'find',\n",
       " 202: 'wanted',\n",
       " 203: 'saw',\n",
       " 204: 'ask',\n",
       " 205: 'man',\n",
       " 206: 'after',\n",
       " 207: 'mother',\n",
       " 208: 'problem',\n",
       " 209: 'alone',\n",
       " 210: 'hear',\n",
       " 211: 'late',\n",
       " 212: 'next',\n",
       " 213: 'ever',\n",
       " 214: 'anyone',\n",
       " 215: 'hard',\n",
       " 216: 'sorry',\n",
       " 217: 'busy',\n",
       " 218: 'their',\n",
       " 219: 'already',\n",
       " 220: 'else',\n",
       " 221: 'door',\n",
       " 222: 'knew',\n",
       " 223: 'nothing',\n",
       " 224: 'boston',\n",
       " 225: 'first',\n",
       " 226: 'other',\n",
       " 227: 'dog',\n",
       " 228: 'thing',\n",
       " 229: 'hope',\n",
       " 230: 'gave',\n",
       " 231: 'couldn',\n",
       " 232: 'call',\n",
       " 233: 'took',\n",
       " 234: 'use',\n",
       " 235: 'bad',\n",
       " 236: 'wasn',\n",
       " 237: 'came',\n",
       " 238: 'best',\n",
       " 239: 'children',\n",
       " 240: 'read',\n",
       " 241: 'mind',\n",
       " 242: 'wrong',\n",
       " 243: 'heard',\n",
       " 244: 'idea',\n",
       " 245: 'afraid',\n",
       " 246: 'being',\n",
       " 247: 'tired',\n",
       " 248: 'getting',\n",
       " 249: 'even',\n",
       " 250: 'aren',\n",
       " 251: 'drink',\n",
       " 252: 'used',\n",
       " 253: 'haven',\n",
       " 254: 'play',\n",
       " 255: 'without',\n",
       " 256: 'often',\n",
       " 257: 'name',\n",
       " 258: 'care',\n",
       " 259: 'friend',\n",
       " 260: 'year',\n",
       " 261: 'morning',\n",
       " 262: 'while',\n",
       " 263: 'remember',\n",
       " 264: 'wants',\n",
       " 265: 'same',\n",
       " 266: 'years',\n",
       " 267: 'looking',\n",
       " 268: 'lost',\n",
       " 269: 'soon',\n",
       " 270: 'ready',\n",
       " 271: 'kind',\n",
       " 272: 'life',\n",
       " 273: 'bought',\n",
       " 274: 'water',\n",
       " 275: 'someone',\n",
       " 276: 'might',\n",
       " 277: 'yourself',\n",
       " 278: 'wait',\n",
       " 279: 'few',\n",
       " 280: 'teacher',\n",
       " 281: 'week',\n",
       " 282: 'bed',\n",
       " 283: 'yet',\n",
       " 284: 'wish',\n",
       " 285: 'party',\n",
       " 286: 'plan',\n",
       " 287: 'parents',\n",
       " 288: 'turn',\n",
       " 289: 'seen',\n",
       " 290: 'happen',\n",
       " 291: 'glad',\n",
       " 292: 'looks',\n",
       " 293: 'place',\n",
       " 294: 'matter',\n",
       " 295: 'found',\n",
       " 296: 'married',\n",
       " 297: 'show',\n",
       " 298: 'nice',\n",
       " 299: 'watch',\n",
       " 300: 'open',\n",
       " 301: 'great',\n",
       " 302: 'bus',\n",
       " 303: 'answer',\n",
       " 304: 'big',\n",
       " 305: 'together',\n",
       " 306: 'myself',\n",
       " 307: 'cold',\n",
       " 308: 'hurt',\n",
       " 309: 'early',\n",
       " 310: 'true',\n",
       " 311: 'shouldn',\n",
       " 312: 'almost',\n",
       " 313: 'another',\n",
       " 314: 'because',\n",
       " 315: 'person',\n",
       " 316: 'knows',\n",
       " 317: 'coming',\n",
       " 318: 'beautiful',\n",
       " 319: 'train',\n",
       " 320: 'both',\n",
       " 321: 'which',\n",
       " 322: 'meeting',\n",
       " 323: 'able',\n",
       " 324: 'those',\n",
       " 325: 'coffee',\n",
       " 326: 'wouldn',\n",
       " 327: 'fun',\n",
       " 328: 'days',\n",
       " 329: 'trying',\n",
       " 330: 'talking',\n",
       " 331: 'doctor',\n",
       " 332: 'felt',\n",
       " 333: 'around',\n",
       " 334: 'anybody',\n",
       " 335: 'everybody',\n",
       " 336: 'once',\n",
       " 337: 'tonight',\n",
       " 338: 'hand',\n",
       " 339: 'most',\n",
       " 340: 'dinner',\n",
       " 341: 'each',\n",
       " 342: 'meet',\n",
       " 343: 'such',\n",
       " 344: 'sleep',\n",
       " 345: 'world',\n",
       " 346: 'seem',\n",
       " 347: 'question',\n",
       " 348: 'careful',\n",
       " 349: 'pay',\n",
       " 350: 'write',\n",
       " 351: 'brother',\n",
       " 352: 'seems',\n",
       " 353: 'books',\n",
       " 354: 'food',\n",
       " 355: 'pretty',\n",
       " 356: 'walk',\n",
       " 357: 'learn',\n",
       " 358: 'nobody',\n",
       " 359: 'study',\n",
       " 360: 'longer',\n",
       " 361: 'english',\n",
       " 362: 'far',\n",
       " 363: 'mine',\n",
       " 364: 'started',\n",
       " 365: 'phone',\n",
       " 366: 'under',\n",
       " 367: 'truth',\n",
       " 368: 'letter',\n",
       " 369: 'cat',\n",
       " 370: 'family',\n",
       " 371: 'tried',\n",
       " 372: 'hours',\n",
       " 373: 'change',\n",
       " 374: 'gone',\n",
       " 375: 'since',\n",
       " 376: 'word',\n",
       " 377: 'start',\n",
       " 378: 'sick',\n",
       " 379: 'fell',\n",
       " 380: 'story',\n",
       " 381: 'finished',\n",
       " 382: 'important',\n",
       " 383: 'forget',\n",
       " 384: 'playing',\n",
       " 385: 'minutes',\n",
       " 386: 'fast',\n",
       " 387: 'hate',\n",
       " 388: 'met',\n",
       " 389: 'waiting',\n",
       " 390: 'own',\n",
       " 391: 'worry',\n",
       " 392: 'wife',\n",
       " 393: 'afternoon',\n",
       " 394: 'broke',\n",
       " 395: 'small',\n",
       " 396: 'anymore',\n",
       " 397: 'close',\n",
       " 398: 'station',\n",
       " 399: 'likes',\n",
       " 400: 'until',\n",
       " 401: 'surprised',\n",
       " 402: 'son',\n",
       " 403: 'movie',\n",
       " 404: 'weather',\n",
       " 405: 'accident',\n",
       " 406: 'guess',\n",
       " 407: 'sister',\n",
       " 408: 'advice',\n",
       " 409: 'run',\n",
       " 410: 'boy',\n",
       " 411: 'looked',\n",
       " 412: 'called',\n",
       " 413: 'young',\n",
       " 414: 'mistake',\n",
       " 415: 'easy',\n",
       " 416: 'times',\n",
       " 417: 'police',\n",
       " 418: 'music',\n",
       " 419: 'wonder',\n",
       " 420: 'difficult',\n",
       " 421: 'visit',\n",
       " 422: 'mean',\n",
       " 423: 'ate',\n",
       " 424: 'listen',\n",
       " 425: 'girl',\n",
       " 426: 'different',\n",
       " 427: 'ten',\n",
       " 428: 'face',\n",
       " 429: 'caught',\n",
       " 430: 'lunch',\n",
       " 431: 'office',\n",
       " 432: 'makes',\n",
       " 433: 'child',\n",
       " 434: 'hungry',\n",
       " 435: 'reading',\n",
       " 436: 'quite',\n",
       " 437: 'working',\n",
       " 438: 'tv',\n",
       " 439: 'become',\n",
       " 440: 'window',\n",
       " 441: 'thank',\n",
       " 442: 'interested',\n",
       " 443: 'exactly',\n",
       " 444: 'decided',\n",
       " 445: 'students',\n",
       " 446: 'tree',\n",
       " 447: 'drive',\n",
       " 448: 'rain',\n",
       " 449: 'trust',\n",
       " 450: 'spend',\n",
       " 451: 'yours',\n",
       " 452: 'hands',\n",
       " 453: 'questions',\n",
       " 454: 'country',\n",
       " 455: 'himself',\n",
       " 456: 'agree',\n",
       " 457: 'bicycle',\n",
       " 458: 'eyes',\n",
       " 459: 'free',\n",
       " 460: 'fire',\n",
       " 461: 'usually',\n",
       " 462: 'kept',\n",
       " 463: 'sing',\n",
       " 464: 'number',\n",
       " 465: 'died',\n",
       " 466: 'finish',\n",
       " 467: 'advised',\n",
       " 468: 'hair',\n",
       " 469: 'proud',\n",
       " 470: 'clock',\n",
       " 471: 'outside',\n",
       " 472: 'needed',\n",
       " 473: 'swim',\n",
       " 474: 'news',\n",
       " 475: 'red',\n",
       " 476: 'supposed',\n",
       " 477: 'summer',\n",
       " 478: 'eating',\n",
       " 479: 'front',\n",
       " 480: 'fish',\n",
       " 481: 'explain',\n",
       " 482: 'having',\n",
       " 483: 'park',\n",
       " 484: 'thinking',\n",
       " 485: 'bit',\n",
       " 486: 'angry',\n",
       " 487: 'arrived',\n",
       " 488: 'somebody',\n",
       " 489: 'interesting',\n",
       " 490: 'full',\n",
       " 491: 'catch',\n",
       " 492: 'invited',\n",
       " 493: 'ran',\n",
       " 494: 'near',\n",
       " 495: 'hat',\n",
       " 496: 'taking',\n",
       " 497: 'maybe',\n",
       " 498: 'australia',\n",
       " 499: 'says',\n",
       " 500: 'trouble',\n",
       " 501: 'hurry',\n",
       " 502: 'sit',\n",
       " 503: 'lives',\n",
       " 504: 'heart',\n",
       " 505: 'end',\n",
       " 506: 'song',\n",
       " 507: 'bring',\n",
       " 508: 'cost',\n",
       " 509: 'breakfast',\n",
       " 510: 'older',\n",
       " 511: 'stupid',\n",
       " 512: 'making',\n",
       " 513: 'crazy',\n",
       " 514: 'born',\n",
       " 515: 'worried',\n",
       " 516: 'daughter',\n",
       " 517: 'box',\n",
       " 518: 'minute',\n",
       " 519: 'studying',\n",
       " 520: 'light',\n",
       " 521: 'moment',\n",
       " 522: 'game',\n",
       " 523: 'hot',\n",
       " 524: 'five',\n",
       " 525: 'high',\n",
       " 526: 'weren',\n",
       " 527: 'favorite',\n",
       " 528: 'behind',\n",
       " 529: 'computer',\n",
       " 530: 'girlfriend',\n",
       " 531: 'picture',\n",
       " 532: 'needs',\n",
       " 533: 'sound',\n",
       " 534: 'stand',\n",
       " 535: 'running',\n",
       " 536: 'reason',\n",
       " 537: 'later',\n",
       " 538: 'miss',\n",
       " 539: 'against',\n",
       " 540: 'rest',\n",
       " 541: 'class',\n",
       " 542: 'month',\n",
       " 543: 'wearing',\n",
       " 544: 'language',\n",
       " 545: 'building',\n",
       " 546: 'cut',\n",
       " 547: 'dictionary',\n",
       " 548: 'rich',\n",
       " 549: 'forgot',\n",
       " 550: 'promise',\n",
       " 551: 'through',\n",
       " 552: 'sat',\n",
       " 553: 'ago',\n",
       " 554: 'asleep',\n",
       " 555: 'dress',\n",
       " 556: 'dogs',\n",
       " 557: 'table',\n",
       " 558: 'allowed',\n",
       " 559: 'win',\n",
       " 560: 'kids',\n",
       " 561: 'boss',\n",
       " 562: 'began',\n",
       " 563: 'wine',\n",
       " 564: 'leaving',\n",
       " 565: 'thanks',\n",
       " 566: 'trip',\n",
       " 567: 'changed',\n",
       " 568: 'then',\n",
       " 569: 'lose',\n",
       " 570: 'paid',\n",
       " 571: 'street',\n",
       " 572: 'scared',\n",
       " 573: 'lie',\n",
       " 574: 'large',\n",
       " 575: 'chance',\n",
       " 576: 'birthday',\n",
       " 577: 'health',\n",
       " 578: 'present',\n",
       " 579: 'thirty',\n",
       " 580: 'watching',\n",
       " 581: 'problems',\n",
       " 582: 'cup',\n",
       " 583: 'whole',\n",
       " 584: 'appreciate',\n",
       " 585: 'living',\n",
       " 586: 'its',\n",
       " 587: 'dance',\n",
       " 588: 'shoes',\n",
       " 589: 'business',\n",
       " 590: 'hospital',\n",
       " 591: 'restaurant',\n",
       " 592: 'hold',\n",
       " 593: 'baby',\n",
       " 594: 'forward',\n",
       " 595: 'spent',\n",
       " 596: 'tall',\n",
       " 597: 'die',\n",
       " 598: 'probably',\n",
       " 599: 'taken',\n",
       " 600: 'japanese',\n",
       " 601: 'expensive',\n",
       " 602: 'saying',\n",
       " 603: 'became',\n",
       " 604: 'homework',\n",
       " 605: 'town',\n",
       " 606: 'known',\n",
       " 607: 'quit',\n",
       " 608: 'sometimes',\n",
       " 609: 'missed',\n",
       " 610: 'eaten',\n",
       " 611: 'six',\n",
       " 612: 'team',\n",
       " 613: 'enjoy',\n",
       " 614: 'student',\n",
       " 615: 'comes',\n",
       " 616: 'safe',\n",
       " 617: 'milk',\n",
       " 618: 'swimming',\n",
       " 619: 'tennis',\n",
       " 620: 'quickly',\n",
       " 621: 'talked',\n",
       " 622: 'dead',\n",
       " 623: 'possible',\n",
       " 624: 'funny',\n",
       " 625: 'closed',\n",
       " 626: 'move',\n",
       " 627: 'choice',\n",
       " 628: 'o',\n",
       " 629: 'break',\n",
       " 630: 'feeling',\n",
       " 631: 'cake',\n",
       " 632: 'serious',\n",
       " 633: 'tea',\n",
       " 634: 'point',\n",
       " 635: 'actually',\n",
       " 636: 'turned',\n",
       " 637: 'guy',\n",
       " 638: 'nervous',\n",
       " 639: 'stayed',\n",
       " 640: 'rules',\n",
       " 641: 'noise',\n",
       " 642: 'seat',\n",
       " 643: 'city',\n",
       " 644: 'real',\n",
       " 645: 'speaking',\n",
       " 646: 'beer',\n",
       " 647: 'drunk',\n",
       " 648: 'shopping',\n",
       " 649: 'hit',\n",
       " 650: 'flowers',\n",
       " 651: 'clothes',\n",
       " 652: 'key',\n",
       " 653: 'store',\n",
       " 654: 'order',\n",
       " 655: 'driving',\n",
       " 656: 'secret',\n",
       " 657: 'kill',\n",
       " 658: 'writing',\n",
       " 659: 'weekend',\n",
       " 660: 'death',\n",
       " 661: 'whether',\n",
       " 662: 'smoking',\n",
       " 663: 'strong',\n",
       " 664: 'gets',\n",
       " 665: 'completely',\n",
       " 666: 'age',\n",
       " 667: 'laugh',\n",
       " 668: 'air',\n",
       " 669: 'necessary',\n",
       " 670: 'killed',\n",
       " 671: 'happens',\n",
       " 672: 'river',\n",
       " 673: 'pain',\n",
       " 674: 'part',\n",
       " 675: 'finally',\n",
       " 676: 'future',\n",
       " 677: 'head',\n",
       " 678: 'monday',\n",
       " 679: 'upset',\n",
       " 680: 'accept',\n",
       " 681: 'goes',\n",
       " 682: 'deal',\n",
       " 683: 'hour',\n",
       " 684: 'beach',\n",
       " 685: 'quiet',\n",
       " 686: 'ok',\n",
       " 687: 'sounds',\n",
       " 688: 'worse',\n",
       " 689: 'men',\n",
       " 690: 'figured',\n",
       " 691: 'whatever',\n",
       " 692: 'hasn',\n",
       " 693: 'snow',\n",
       " 694: 'inside',\n",
       " 695: 'dark',\n",
       " 696: 'happening',\n",
       " 697: 'situation',\n",
       " 698: 'between',\n",
       " 699: 'takes',\n",
       " 700: 'war',\n",
       " 701: 'list',\n",
       " 702: 'japan',\n",
       " 703: 'seeing',\n",
       " 704: 'worked',\n",
       " 705: 'either',\n",
       " 706: 'evening',\n",
       " 707: 'satisfied',\n",
       " 708: 'grow',\n",
       " 709: 'paper',\n",
       " 710: 'helped',\n",
       " 711: 'words',\n",
       " 712: 'dangerous',\n",
       " 713: 'loved',\n",
       " 714: 'works',\n",
       " 715: 'opinion',\n",
       " 716: 'ahead',\n",
       " 717: 'company',\n",
       " 718: 'lying',\n",
       " 719: 'speaks',\n",
       " 720: 'perfect',\n",
       " 721: 'medicine',\n",
       " 722: 'beginning',\n",
       " 723: 'handle',\n",
       " 724: 'woman',\n",
       " 725: 'broken',\n",
       " 726: 'dream',\n",
       " 727: 'involved',\n",
       " 728: 'lived',\n",
       " 729: 'along',\n",
       " 730: 'loves',\n",
       " 731: 'decision',\n",
       " 732: 'wonderful',\n",
       " 733: 'stopped',\n",
       " 734: 'half',\n",
       " 735: 'wrote',\n",
       " 736: 'apologize',\n",
       " 737: 'earth',\n",
       " 738: 'sad',\n",
       " 739: 'borrow',\n",
       " 740: 'telling',\n",
       " 741: 'library',\n",
       " 742: 'difference',\n",
       " 743: 'touch',\n",
       " 744: 'husband',\n",
       " 745: 'smell',\n",
       " 746: 'rather',\n",
       " 747: 'others',\n",
       " 748: 'excuse',\n",
       " 749: 'plane',\n",
       " 750: 'gun',\n",
       " 751: 'boyfriend',\n",
       " 752: 'short',\n",
       " 753: 'promised',\n",
       " 754: 'sense',\n",
       " 755: 'disappointed',\n",
       " 756: 'coat',\n",
       " 757: 'hotel',\n",
       " 758: 'decide',\n",
       " 759: 'cannot',\n",
       " 760: 'wake',\n",
       " 761: 'plenty',\n",
       " 762: 'liked',\n",
       " 763: 'strange',\n",
       " 764: 'weight',\n",
       " 765: 'spoke',\n",
       " 766: 'piano',\n",
       " 767: 'slept',\n",
       " 768: 'waste',\n",
       " 769: 'save',\n",
       " 770: 'drinking',\n",
       " 771: 'ice',\n",
       " 772: 'uncle',\n",
       " 773: 'prefer',\n",
       " 774: 'carefully',\n",
       " 775: 'mad',\n",
       " 776: 'second',\n",
       " 777: 'join',\n",
       " 778: 'road',\n",
       " 779: 'message',\n",
       " 780: 'waited',\n",
       " 781: 'information',\n",
       " 782: 'listening',\n",
       " 783: 'shut',\n",
       " 784: 'less',\n",
       " 785: 'dollars',\n",
       " 786: 'white',\n",
       " 787: 'sign',\n",
       " 788: 'whose',\n",
       " 789: 'luck',\n",
       " 790: 'learned',\n",
       " 791: 'shirt',\n",
       " 792: 'lawyer',\n",
       " 793: 'held',\n",
       " 794: 'keys',\n",
       " 795: 'owe',\n",
       " 796: 'warm',\n",
       " 797: 'mistakes',\n",
       " 798: 'glasses',\n",
       " 799: 'suppose',\n",
       " 800: 'floor',\n",
       " 801: 'showed',\n",
       " 802: 'passed',\n",
       " 803: 'clean',\n",
       " 804: 'side',\n",
       " 805: 'sleeping',\n",
       " 806: 'lake',\n",
       " 807: 'worth',\n",
       " 808: 'bank',\n",
       " 809: 'piece',\n",
       " 810: 'somewhere',\n",
       " 811: 'patient',\n",
       " 812: 'empty',\n",
       " 813: 'check',\n",
       " 814: 'poor',\n",
       " 815: 'regret',\n",
       " 816: 'chair',\n",
       " 817: 'leaves',\n",
       " 818: 'crying',\n",
       " 819: 'test',\n",
       " 820: 'walked',\n",
       " 821: 'offer',\n",
       " 822: 'camera',\n",
       " 823: 'convinced',\n",
       " 824: 'joke',\n",
       " 825: 'standing',\n",
       " 826: 'guys',\n",
       " 827: 'blame',\n",
       " 828: 'sun',\n",
       " 829: 'brought',\n",
       " 830: 'lucky',\n",
       " 831: 'line',\n",
       " 832: 'shot',\n",
       " 833: 'favor',\n",
       " 834: 'impossible',\n",
       " 835: 'telephone',\n",
       " 836: 'herself',\n",
       " 837: 'famous',\n",
       " 838: 'experience',\n",
       " 839: 'traffic',\n",
       " 840: 'smart',\n",
       " 841: 'danger',\n",
       " 842: 'girls',\n",
       " 843: 'drank',\n",
       " 844: 'ideas',\n",
       " 845: 'written',\n",
       " 846: 'drop',\n",
       " 847: 'fine',\n",
       " 848: 'during',\n",
       " 849: 'follow',\n",
       " 850: 'hadn',\n",
       " 851: 'ride',\n",
       " 852: 'sitting',\n",
       " 853: 'sent',\n",
       " 854: 'across',\n",
       " 855: 'dropped',\n",
       " 856: 'four',\n",
       " 857: 'given',\n",
       " 858: 'president',\n",
       " 859: 'television',\n",
       " 860: 'mountain',\n",
       " 861: 'desk',\n",
       " 862: 'sugar',\n",
       " 863: 'concert',\n",
       " 864: 'wash',\n",
       " 865: 'case',\n",
       " 866: 'wear',\n",
       " 867: 'taught',\n",
       " 868: 'walking',\n",
       " 869: 'choose',\n",
       " 870: 'cooking',\n",
       " 871: 'count',\n",
       " 872: 'fall',\n",
       " 873: 'glass',\n",
       " 874: 'fight',\n",
       " 875: 'animals',\n",
       " 876: 'discuss',\n",
       " 877: 'meal',\n",
       " 878: 'staying',\n",
       " 879: 'expect',\n",
       " 880: 'afford',\n",
       " 881: 'liar',\n",
       " 882: 'kiss',\n",
       " 883: 'anywhere',\n",
       " 884: 'opened',\n",
       " 885: 'arrive',\n",
       " 886: 'least',\n",
       " 887: 'willing',\n",
       " 888: 'admit',\n",
       " 889: 'received',\n",
       " 890: 'color',\n",
       " 891: 'jealous',\n",
       " 892: 'success',\n",
       " 893: 'share',\n",
       " 894: 'expected',\n",
       " 895: 'law',\n",
       " 896: 'john',\n",
       " 897: 'send',\n",
       " 898: 'fair',\n",
       " 899: 'public',\n",
       " 900: 'fired',\n",
       " 901: 'ticket',\n",
       " 902: 'lock',\n",
       " 903: 'fault',\n",
       " 904: 'duty',\n",
       " 905: 'guitar',\n",
       " 906: 'lend',\n",
       " 907: 'raining',\n",
       " 908: 'kid',\n",
       " 909: 'cry',\n",
       " 910: 'kitchen',\n",
       " 911: 'enemy',\n",
       " 912: 'past',\n",
       " 913: 'radio',\n",
       " 914: 'umbrella',\n",
       " 915: 'ship',\n",
       " 916: 'driver',\n",
       " 917: 'plans',\n",
       " 918: 'excited',\n",
       " 919: 'pick',\n",
       " 920: 'clear',\n",
       " 921: 'several',\n",
       " 922: 'cute',\n",
       " 923: 'solve',\n",
       " 924: 'grandfather',\n",
       " 925: 'painting',\n",
       " 926: 'cool',\n",
       " 927: 'stuff',\n",
       " 928: 'set',\n",
       " 929: 'native',\n",
       " 930: 'instead',\n",
       " 931: 'weird',\n",
       " 932: 'missing',\n",
       " 933: 'women',\n",
       " 934: 'laughed',\n",
       " 935: 'considered',\n",
       " 936: 'price',\n",
       " 937: 'meat',\n",
       " 938: 'shop',\n",
       " 939: 'sentence',\n",
       " 940: 'address',\n",
       " 941: 'earlier',\n",
       " 942: 'ordered',\n",
       " 943: 'cook',\n",
       " 944: 'attention',\n",
       " 945: 'tokyo',\n",
       " 946: 'consider',\n",
       " 947: 'arm',\n",
       " 948: 'bag',\n",
       " 949: 'thinks',\n",
       " 950: 'report',\n",
       " 951: 'date',\n",
       " 952: 'sunday',\n",
       " 953: 'singer',\n",
       " 954: 'gotten',\n",
       " 955: 'america',\n",
       " 956: 'means',\n",
       " 957: 'likely',\n",
       " 958: 'sold',\n",
       " 959: 'starting',\n",
       " 960: 'novel',\n",
       " 961: 'though',\n",
       " 962: 'recently',\n",
       " 963: 'throw',\n",
       " 964: 'younger',\n",
       " 965: 'succeed',\n",
       " 966: 'harder',\n",
       " 967: 'calling',\n",
       " 968: 'eight',\n",
       " 969: 'charge',\n",
       " 970: 'asking',\n",
       " 971: 'boys',\n",
       " 972: 'played',\n",
       " 973: 'accused',\n",
       " 974: 'garden',\n",
       " 975: 'airport',\n",
       " 976: 'college',\n",
       " 977: 'boring',\n",
       " 978: 'wind',\n",
       " 979: 'step',\n",
       " 980: 'locked',\n",
       " 981: 'dressed',\n",
       " 982: 'gift',\n",
       " 983: 'seemed',\n",
       " 984: 'hardly',\n",
       " 985: 'government',\n",
       " 986: 'chocolate',\n",
       " 987: 'cats',\n",
       " 988: 'sell',\n",
       " 989: 'machine',\n",
       " 990: 'smoke',\n",
       " 991: 'prison',\n",
       " 992: 'correct',\n",
       " 993: 'alive',\n",
       " 994: 'months',\n",
       " 995: 'teach',\n",
       " 996: 'pen',\n",
       " 997: 'certain',\n",
       " 998: 'canadian',\n",
       " 999: 'calm',\n",
       " 1000: 'hundred',\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d84079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2334, 158, 128, 33, 41, 3, 2],\n",
       " [1, 6119, 30, 2125, 60, 43, 2],\n",
       " [1, 6120, 11, 401, 1076, 3006, 74, 4467, 25, 365, 13, 751, 4468, 3, 2],\n",
       " [1, 17, 56, 30, 1785, 6, 2],\n",
       " [1, 1557, 25, 20, 26, 1558, 54, 18, 6121, 3, 2],\n",
       " [1, 8, 22, 10, 9, 31, 3565, 6, 2],\n",
       " [1, 28, 618, 118, 13, 1676, 3, 2],\n",
       " [1, 28, 21, 4469, 93, 19, 3, 2],\n",
       " [1, 91, 6122, 17, 185, 18, 6, 2],\n",
       " [1, 2608, 1150, 5, 644, 25, 21, 8, 22, 7, 6, 2]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(filters=' ')   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(fra_corpus)                 # fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(fra_corpus)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a40a7849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<sos>',\n",
       " 2: '<eos>',\n",
       " 3: '.',\n",
       " 4: 'je',\n",
       " 5: 'de',\n",
       " 6: '?',\n",
       " 7: 'pas',\n",
       " 8: 'est',\n",
       " 9: 'vous',\n",
       " 10: 'que',\n",
       " 11: 'a',\n",
       " 12: 'il',\n",
       " 13: 'le',\n",
       " 14: 'ne',\n",
       " 15: 'd',\n",
       " 16: 'la',\n",
       " 17: 'tu',\n",
       " 18: 'l',\n",
       " 19: 'tom',\n",
       " 20: 'j',\n",
       " 21: 'n',\n",
       " 22: 'ce',\n",
       " 23: 's',\n",
       " 24: 'un',\n",
       " 25: ',',\n",
       " 26: 'ai',\n",
       " 27: 't',\n",
       " 28: 'nous',\n",
       " 29: 'en',\n",
       " 30: 'une',\n",
       " 31: 'me',\n",
       " 32: 'les',\n",
       " 33: 'm',\n",
       " 34: 'c',\n",
       " 35: 'suis',\n",
       " 36: 'pour',\n",
       " 37: 'e',\n",
       " 38: 'te',\n",
       " 39: 'qu',\n",
       " 40: 'elle',\n",
       " 41: 're',\n",
       " 42: 'faire',\n",
       " 43: '!',\n",
       " 44: 'r',\n",
       " 45: 'des',\n",
       " 46: 'dans',\n",
       " 47: 'plus',\n",
       " 48: 'tes',\n",
       " 49: 'tre',\n",
       " 50: 'es',\n",
       " 51: 'qui',\n",
       " 52: 'moi',\n",
       " 53: 'y',\n",
       " 54: 'tout',\n",
       " 55: 'tait',\n",
       " 56: 'veux',\n",
       " 57: 'du',\n",
       " 58: 'avec',\n",
       " 59: 'au',\n",
       " 60: 'se',\n",
       " 61: 'fait',\n",
       " 62: 'mon',\n",
       " 63: 'si',\n",
       " 64: 'as',\n",
       " 65: 'et',\n",
       " 66: 'pr',\n",
       " 67: 'sont',\n",
       " 68: 'avez',\n",
       " 69: 'ils',\n",
       " 70: 'on',\n",
       " 71: 'cette',\n",
       " 72: 'tr',\n",
       " 73: 'peux',\n",
       " 74: 'sur',\n",
       " 75: 'cela',\n",
       " 76: 'votre',\n",
       " 77: 'ici',\n",
       " 78: 'son',\n",
       " 79: 'temps',\n",
       " 80: 'dit',\n",
       " 81: 'ma',\n",
       " 82: 'sais',\n",
       " 83: 'lui',\n",
       " 84: 'pourquoi',\n",
       " 85: 'chose',\n",
       " 86: 'pense',\n",
       " 87: 'toi',\n",
       " 88: 'o',\n",
       " 89: 'ton',\n",
       " 90: 'mes',\n",
       " 91: 'comment',\n",
       " 92: 'vraiment',\n",
       " 93: 'jamais',\n",
       " 94: 'par',\n",
       " 95: 'quelque',\n",
       " 96: 'avons',\n",
       " 97: 'bien',\n",
       " 98: 'beaucoup',\n",
       " 99: 'ont',\n",
       " 100: 'elles',\n",
       " 101: 'aime',\n",
       " 102: 'personne',\n",
       " 103: 'sa',\n",
       " 104: 'tous',\n",
       " 105: 'besoin',\n",
       " 106: 'dire',\n",
       " 107: 'aller',\n",
       " 108: 'avoir',\n",
       " 109: 'peut',\n",
       " 110: 'sommes',\n",
       " 111: 'p',\n",
       " 112: 'tais',\n",
       " 113: 'parler',\n",
       " 114: 'maison',\n",
       " 115: 'va',\n",
       " 116: 'peu',\n",
       " 117: 'quand',\n",
       " 118: 'voir',\n",
       " 119: 'encore',\n",
       " 120: 'rien',\n",
       " 121: 'v',\n",
       " 122: 'comme',\n",
       " 123: 'trop',\n",
       " 124: 'vais',\n",
       " 125: 'deux',\n",
       " 126: 'monde',\n",
       " 127: 'toujours',\n",
       " 128: 'ta',\n",
       " 129: 'train',\n",
       " 130: 'dois',\n",
       " 131: 'mary',\n",
       " 132: 'combien',\n",
       " 133: 'autre',\n",
       " 134: 'argent',\n",
       " 135: 'fran',\n",
       " 136: 'f',\n",
       " 137: 'ais',\n",
       " 138: 'quoi',\n",
       " 139: 'avait',\n",
       " 140: 'soit',\n",
       " 141: 'mais',\n",
       " 142: 'voiture',\n",
       " 143: 'chez',\n",
       " 144: 'aimerais',\n",
       " 145: 'air',\n",
       " 146: 'arr',\n",
       " 147: 'quelle',\n",
       " 148: 'travail',\n",
       " 149: 'avais',\n",
       " 150: 'ses',\n",
       " 151: 'fois',\n",
       " 152: 'hier',\n",
       " 153: 'faut',\n",
       " 154: 'vu',\n",
       " 155: 'ces',\n",
       " 156: 'bon',\n",
       " 157: 'fais',\n",
       " 158: 'ceci',\n",
       " 159: 'aider',\n",
       " 160: 'g',\n",
       " 161: 'juste',\n",
       " 162: 'quel',\n",
       " 163: 'aujourd',\n",
       " 164: 'hui',\n",
       " 165: 'probl',\n",
       " 166: 'apr',\n",
       " 167: 'pouvez',\n",
       " 168: 'aussi',\n",
       " 169: 'maintenant',\n",
       " 170: 'quelqu',\n",
       " 171: 'bonne',\n",
       " 172: 'arrive',\n",
       " 173: 'soir',\n",
       " 174: 'eu',\n",
       " 175: 'heures',\n",
       " 176: 'voulez',\n",
       " 177: 'mal',\n",
       " 178: 'pensais',\n",
       " 179: 'marie',\n",
       " 180: 'demain',\n",
       " 181: 'porte',\n",
       " 182: 'assez',\n",
       " 183: 'pass',\n",
       " 184: 'notre',\n",
       " 185: 'no',\n",
       " 186: 'manger',\n",
       " 187: 'toutes',\n",
       " 188: 'gens',\n",
       " 189: 'livre',\n",
       " 190: 'mieux',\n",
       " 191: 'prendre',\n",
       " 192: 'devrais',\n",
       " 193: 'sans',\n",
       " 194: 'enfants',\n",
       " 195: 'h',\n",
       " 196: 'pla',\n",
       " 197: 'heure',\n",
       " 198: 'puis',\n",
       " 199: 'avant',\n",
       " 200: 'esp',\n",
       " 201: 'b',\n",
       " 202: 'all',\n",
       " 203: 'cole',\n",
       " 204: 'aux',\n",
       " 205: 'id',\n",
       " 206: 'int',\n",
       " 207: 'trois',\n",
       " 208: 'ou',\n",
       " 209: 'moment',\n",
       " 210: 'aide',\n",
       " 211: 'grand',\n",
       " 212: 'choses',\n",
       " 213: 'sol',\n",
       " 214: 'seul',\n",
       " 215: 'toute',\n",
       " 216: 'pris',\n",
       " 217: 'occup',\n",
       " 218: 'venir',\n",
       " 219: 'veut',\n",
       " 220: 'derni',\n",
       " 221: 'partir',\n",
       " 222: 'homme',\n",
       " 223: 'sait',\n",
       " 224: 'cet',\n",
       " 225: 'savoir',\n",
       " 226: 'petit',\n",
       " 227: 'aurais',\n",
       " 228: 'nouveau',\n",
       " 229: 'quelques',\n",
       " 230: 'ter',\n",
       " 231: 'jour',\n",
       " 232: 'fut',\n",
       " 233: 'rester',\n",
       " 234: 'vos',\n",
       " 235: 'boston',\n",
       " 236: 'part',\n",
       " 237: 'arriv',\n",
       " 238: 'croire',\n",
       " 239: 'nuit',\n",
       " 240: 'pendant',\n",
       " 241: 'appr',\n",
       " 242: 'raison',\n",
       " 243: 'chien',\n",
       " 244: 'ann',\n",
       " 245: 'sois',\n",
       " 246: 'jours',\n",
       " 247: 'rendre',\n",
       " 248: 'demand',\n",
       " 249: 'amis',\n",
       " 250: 'achet',\n",
       " 251: 'ur',\n",
       " 252: 'peur',\n",
       " 253: 'simplement',\n",
       " 254: 'sens',\n",
       " 255: 'heureux',\n",
       " 256: 'eau',\n",
       " 257: 'depuis',\n",
       " 258: 'pens',\n",
       " 259: 'vie',\n",
       " 260: 'laisse',\n",
       " 261: 'rent',\n",
       " 262: 'pu',\n",
       " 263: 'ans',\n",
       " 264: 'leur',\n",
       " 265: 'voulais',\n",
       " 266: 'savais',\n",
       " 267: 'crois',\n",
       " 268: 'pouvons',\n",
       " 269: 'acheter',\n",
       " 270: 'seule',\n",
       " 271: 'dis',\n",
       " 272: 'parle',\n",
       " 273: 'difficile',\n",
       " 274: 'entendu',\n",
       " 275: 'fr',\n",
       " 276: 'taient',\n",
       " 277: 'semaine',\n",
       " 278: 'pi',\n",
       " 279: 'jusqu',\n",
       " 280: 'faites',\n",
       " 281: 'parents',\n",
       " 282: 'souvent',\n",
       " 283: 'semble',\n",
       " 284: 'ami',\n",
       " 285: 'retard',\n",
       " 286: 'matin',\n",
       " 287: 'nom',\n",
       " 288: 'pourrais',\n",
       " 289: 'demande',\n",
       " 290: 'soyez',\n",
       " 291: 'fatigu',\n",
       " 292: 'main',\n",
       " 293: 'allez',\n",
       " 294: 'non',\n",
       " 295: 'pourrait',\n",
       " 296: 'chambre',\n",
       " 297: 'question',\n",
       " 298: 'fille',\n",
       " 299: 'donn',\n",
       " 300: 'ci',\n",
       " 301: 'res',\n",
       " 302: 'devons',\n",
       " 303: 'jeuner',\n",
       " 304: 'viens',\n",
       " 305: 'diff',\n",
       " 306: 'gar',\n",
       " 307: 'presque',\n",
       " 308: 'penses',\n",
       " 309: 'lit',\n",
       " 310: 'sera',\n",
       " 311: 'perdu',\n",
       " 312: 'passe',\n",
       " 313: 'bus',\n",
       " 314: 'allons',\n",
       " 315: 'moins',\n",
       " 316: 'grande',\n",
       " 317: 'lorsque',\n",
       " 318: 'tard',\n",
       " 319: 'trouv',\n",
       " 320: 'reste',\n",
       " 321: 'aucun',\n",
       " 322: 'accord',\n",
       " 323: 'sous',\n",
       " 324: 'journ',\n",
       " 325: 'aucune',\n",
       " 326: 'caf',\n",
       " 327: 'nouvelle',\n",
       " 328: 'longtemps',\n",
       " 329: 'femme',\n",
       " 330: 'prie',\n",
       " 331: 'phone',\n",
       " 332: 'coup',\n",
       " 333: 'laissez',\n",
       " 334: 'vas',\n",
       " 335: 'entendre',\n",
       " 336: 'adore',\n",
       " 337: 'fort',\n",
       " 338: 'sujet',\n",
       " 339: 'vrai',\n",
       " 340: 'sortir',\n",
       " 341: 'devrait',\n",
       " 342: 'jouer',\n",
       " 343: 'trouve',\n",
       " 344: 'entre',\n",
       " 345: 'mari',\n",
       " 346: 'vite',\n",
       " 347: 'veuillez',\n",
       " 348: 'cher',\n",
       " 349: 'doit',\n",
       " 350: 'autres',\n",
       " 351: 'rit',\n",
       " 352: 'voulait',\n",
       " 353: 'tiez',\n",
       " 354: 'dites',\n",
       " 355: 'ensemble',\n",
       " 356: 'travailler',\n",
       " 357: 'laiss',\n",
       " 358: 'trouver',\n",
       " 359: 'co',\n",
       " 360: 'davantage',\n",
       " 361: 'devriez',\n",
       " 362: 'histoire',\n",
       " 363: 'pouvoir',\n",
       " 364: 'chaque',\n",
       " 365: 'dont',\n",
       " 366: 'venu',\n",
       " 367: 'anglais',\n",
       " 368: 'essayer',\n",
       " 369: 'livres',\n",
       " 370: 'mauvais',\n",
       " 371: 'point',\n",
       " 372: 'am',\n",
       " 373: 'donner',\n",
       " 374: 'nager',\n",
       " 375: 'ge',\n",
       " 376: 'vivre',\n",
       " 377: 'ch',\n",
       " 378: 'pensez',\n",
       " 379: 've',\n",
       " 380: 'devez',\n",
       " 381: 'voudrais',\n",
       " 382: 'comprends',\n",
       " 383: 'mis',\n",
       " 384: 'attendre',\n",
       " 385: 'mang',\n",
       " 386: 'enfant',\n",
       " 387: 'famille',\n",
       " 388: 'ville',\n",
       " 389: 'personnes',\n",
       " 390: 'donne',\n",
       " 391: 'u',\n",
       " 392: 'tellement',\n",
       " 393: 'seulement',\n",
       " 394: 'merci',\n",
       " 395: 'lire',\n",
       " 396: 'vient',\n",
       " 397: 'union',\n",
       " 398: 'connais',\n",
       " 399: 'midi',\n",
       " 400: 'alors',\n",
       " 401: 'dix',\n",
       " 402: 'serai',\n",
       " 403: 'lettre',\n",
       " 404: 'plan',\n",
       " 405: 'tant',\n",
       " 406: 'vois',\n",
       " 407: 'cl',\n",
       " 408: 'jeune',\n",
       " 409: 'crire',\n",
       " 410: 'apprendre',\n",
       " 411: 'rencontr',\n",
       " 412: 'mort',\n",
       " 413: 'froid',\n",
       " 414: 'minutes',\n",
       " 415: 'meilleur',\n",
       " 416: 'heureuse',\n",
       " 417: 'prends',\n",
       " 418: 'col',\n",
       " 419: 'bureau',\n",
       " 420: 'petite',\n",
       " 421: 'fils',\n",
       " 422: 'pays',\n",
       " 423: 'vieux',\n",
       " 424: 'passer',\n",
       " 425: 'accident',\n",
       " 426: 'ignore',\n",
       " 427: 'pourriez',\n",
       " 428: 'oubli',\n",
       " 429: 'place',\n",
       " 430: 'chat',\n",
       " 431: 'mot',\n",
       " 432: 'mois',\n",
       " 433: 'serait',\n",
       " 434: 'demander',\n",
       " 435: 'malade',\n",
       " 436: 'fen',\n",
       " 437: 'peuvent',\n",
       " 438: 'savez',\n",
       " 439: 'regarder',\n",
       " 440: 'police',\n",
       " 441: 'devrions',\n",
       " 442: 'aimes',\n",
       " 443: 'conna',\n",
       " 444: 'chance',\n",
       " 445: 'faim',\n",
       " 446: 'belle',\n",
       " 447: 'tudier',\n",
       " 448: 'feu',\n",
       " 449: 'ainsi',\n",
       " 450: 'bas',\n",
       " 451: 'erreur',\n",
       " 452: 'celui',\n",
       " 453: 'parl',\n",
       " 454: 'lo',\n",
       " 455: 'yeux',\n",
       " 456: 'regarde',\n",
       " 457: 'endroit',\n",
       " 458: 'termin',\n",
       " 459: 'plut',\n",
       " 460: 'dernier',\n",
       " 461: 'photo',\n",
       " 462: 'laisser',\n",
       " 463: 'teste',\n",
       " 464: 'nourriture',\n",
       " 465: 'comprendre',\n",
       " 466: 'devant',\n",
       " 467: 'attention',\n",
       " 468: 'film',\n",
       " 469: 'cid',\n",
       " 470: 'appel',\n",
       " 471: 'suppose',\n",
       " 472: 'musique',\n",
       " 473: 'inqui',\n",
       " 474: 'th',\n",
       " 475: 'arriver',\n",
       " 476: 'mani',\n",
       " 477: 'affaire',\n",
       " 478: 'envie',\n",
       " 479: 'commenc',\n",
       " 480: 'tomb',\n",
       " 481: 'lieu',\n",
       " 482: 'partie',\n",
       " 483: 'compte',\n",
       " 484: 'confiance',\n",
       " 485: 'vont',\n",
       " 486: 'penser',\n",
       " 487: 'exactement',\n",
       " 488: 'professeur',\n",
       " 489: 'bient',\n",
       " 490: 'premier',\n",
       " 491: 'decin',\n",
       " 492: 'possible',\n",
       " 493: 'agit',\n",
       " 494: 'aurait',\n",
       " 495: 'bless',\n",
       " 496: 'tomber',\n",
       " 497: 'invit',\n",
       " 498: 'gare',\n",
       " 499: 'serais',\n",
       " 500: 'cie',\n",
       " 501: 'dehors',\n",
       " 502: 'loin',\n",
       " 503: 'conduire',\n",
       " 504: 'crit',\n",
       " 505: 'fini',\n",
       " 506: 'autant',\n",
       " 507: 'terre',\n",
       " 508: 'premi',\n",
       " 509: 'questions',\n",
       " 510: 'chemin',\n",
       " 511: 'nos',\n",
       " 512: 'ro',\n",
       " 513: 'cours',\n",
       " 514: 'mettre',\n",
       " 515: 'rieur',\n",
       " 516: 'beau',\n",
       " 517: 'tions',\n",
       " 518: 'assis',\n",
       " 519: 'chaud',\n",
       " 520: 'cr',\n",
       " 521: 'dormir',\n",
       " 522: 'rest',\n",
       " 523: 'genre',\n",
       " 524: 'sant',\n",
       " 525: 'fl',\n",
       " 526: 'ait',\n",
       " 527: 'montre',\n",
       " 528: 'essay',\n",
       " 529: 'ner',\n",
       " 530: 'voyage',\n",
       " 531: 'che',\n",
       " 532: 'leurs',\n",
       " 533: 'prochaine',\n",
       " 534: 'amie',\n",
       " 535: 'utiliser',\n",
       " 536: 'cheveux',\n",
       " 537: 'montrer',\n",
       " 538: 'ferais',\n",
       " 539: 'eux',\n",
       " 540: 'puisse',\n",
       " 541: 'verre',\n",
       " 542: 'prenez',\n",
       " 543: 'dessus',\n",
       " 544: 'vit',\n",
       " 545: 'rencontrer',\n",
       " 546: 'surpris',\n",
       " 547: 'commence',\n",
       " 548: 'dur',\n",
       " 549: 'cadeau',\n",
       " 550: 'langue',\n",
       " 551: 'vue',\n",
       " 552: 'boire',\n",
       " 553: 'visite',\n",
       " 554: 'prix',\n",
       " 555: 'chanter',\n",
       " 556: 'compl',\n",
       " 557: 'plait',\n",
       " 558: 'contre',\n",
       " 559: 'ponse',\n",
       " 560: 'payer',\n",
       " 561: 'content',\n",
       " 562: 'ons',\n",
       " 563: 'parti',\n",
       " 564: 'savait',\n",
       " 565: 'facile',\n",
       " 566: 'pouvais',\n",
       " 567: 'habitude',\n",
       " 568: 'quiconque',\n",
       " 569: 'parer',\n",
       " 570: 'choix',\n",
       " 571: 'lu',\n",
       " 572: 'pouvait',\n",
       " 573: 'australie',\n",
       " 574: 'mains',\n",
       " 575: 'num',\n",
       " 576: 'joue',\n",
       " 577: 'important',\n",
       " 578: 'appris',\n",
       " 579: 'cinq',\n",
       " 580: 'chapeau',\n",
       " 581: 'boulot',\n",
       " 582: 'riche',\n",
       " 583: 'mange',\n",
       " 584: 'marcher',\n",
       " 585: 'fin',\n",
       " 586: 'produit',\n",
       " 587: 'fa',\n",
       " 588: 'pondre',\n",
       " 589: 'travaille',\n",
       " 590: 'ordinateur',\n",
       " 591: 'teau',\n",
       " 592: 'faisait',\n",
       " 593: 'derri',\n",
       " 594: 'vol',\n",
       " 595: 'bois',\n",
       " 596: 'ress',\n",
       " 597: 'intention',\n",
       " 598: 'emploi',\n",
       " 599: 'mets',\n",
       " 600: 'rapidement',\n",
       " 601: 'importe',\n",
       " 602: 'nombreux',\n",
       " 603: 'sent',\n",
       " 604: 'rement',\n",
       " 605: 'devenir',\n",
       " 606: 'dictionnaire',\n",
       " 607: 'rer',\n",
       " 608: 'probablement',\n",
       " 609: 'rouge',\n",
       " 610: 'lumi',\n",
       " 611: 'tel',\n",
       " 612: 'perdre',\n",
       " 613: 'vouloir',\n",
       " 614: 'go',\n",
       " 615: 'entrer',\n",
       " 616: 'revoir',\n",
       " 617: 'furent',\n",
       " 618: 'voulons',\n",
       " 619: 'chanson',\n",
       " 620: 'vers',\n",
       " 621: 'tudiants',\n",
       " 622: 'essaie',\n",
       " 623: 'tement',\n",
       " 624: 'vin',\n",
       " 625: 'oblig',\n",
       " 626: 'anniversaire',\n",
       " 627: 'bo',\n",
       " 628: 'peine',\n",
       " 629: 'minute',\n",
       " 630: 'ferm',\n",
       " 631: 'rentrer',\n",
       " 632: 'poser',\n",
       " 633: 'vision',\n",
       " 634: 'vieille',\n",
       " 635: 'propos',\n",
       " 636: 'gros',\n",
       " 637: 'exp',\n",
       " 638: 'plein',\n",
       " 639: 'mauvaise',\n",
       " 640: 'rue',\n",
       " 641: 'lorsqu',\n",
       " 642: 'appelle',\n",
       " 643: 'attends',\n",
       " 644: 'soleil',\n",
       " 645: 'fi',\n",
       " 646: 'restaurant',\n",
       " 647: 'poste',\n",
       " 648: 'cessaire',\n",
       " 649: 'bruit',\n",
       " 650: 'quipe',\n",
       " 651: 'chercher',\n",
       " 652: 'retour',\n",
       " 653: 'ceux',\n",
       " 654: 'chiens',\n",
       " 655: 'cass',\n",
       " 656: 'table',\n",
       " 657: 'changer',\n",
       " 658: 'lundi',\n",
       " 659: 'celle',\n",
       " 660: 'ment',\n",
       " 661: 'feriez',\n",
       " 662: 'pital',\n",
       " 663: 'trente',\n",
       " 664: 'robe',\n",
       " 665: 'droit',\n",
       " 666: 'auparavant',\n",
       " 667: 'fier',\n",
       " 668: 'long',\n",
       " 669: 'arbre',\n",
       " 670: 'pleurer',\n",
       " 671: 'aimez',\n",
       " 672: 'faisons',\n",
       " 673: 'parce',\n",
       " 674: 'imm',\n",
       " 675: 'veulent',\n",
       " 676: 'avion',\n",
       " 677: 'font',\n",
       " 678: 'prochain',\n",
       " 679: 'magasin',\n",
       " 680: 'quatre',\n",
       " 681: 'secret',\n",
       " 682: 'voulu',\n",
       " 683: 'fasse',\n",
       " 684: 'chang',\n",
       " 685: 'bi',\n",
       " 686: 'fou',\n",
       " 687: 'rence',\n",
       " 688: 'tez',\n",
       " 689: 'cuisine',\n",
       " 690: 'repas',\n",
       " 691: 'rire',\n",
       " 692: 'soin',\n",
       " 693: 'gles',\n",
       " 694: 'devoir',\n",
       " 695: 'lev',\n",
       " 696: 'dispose',\n",
       " 697: 'route',\n",
       " 698: 'avis',\n",
       " 699: 'veill',\n",
       " 700: 'neige',\n",
       " 701: 'porter',\n",
       " 702: 'pay',\n",
       " 703: 'allait',\n",
       " 704: 'travers',\n",
       " 705: 'sorte',\n",
       " 706: 'journal',\n",
       " 707: 'entends',\n",
       " 708: 'capable',\n",
       " 709: 'aim',\n",
       " 710: 'tasse',\n",
       " 711: 'classe',\n",
       " 712: 'regard',\n",
       " 713: 'rentr',\n",
       " 714: 'tennis',\n",
       " 715: 'laquelle',\n",
       " 716: 'plaisir',\n",
       " 717: 'diatement',\n",
       " 718: 'guerre',\n",
       " 719: 'devoirs',\n",
       " 720: 'fit',\n",
       " 721: 'fleurs',\n",
       " 722: 'instant',\n",
       " 723: 'voudriez',\n",
       " 724: 'ayez',\n",
       " 725: 'appeler',\n",
       " 726: 'mots',\n",
       " 727: 'hommes',\n",
       " 728: 'lait',\n",
       " 729: 'garder',\n",
       " 730: 'curit',\n",
       " 731: 'examen',\n",
       " 732: 'cause',\n",
       " 733: 'chaussures',\n",
       " 734: 'bu',\n",
       " 735: 'nouvelles',\n",
       " 736: 'souviens',\n",
       " 737: 'finir',\n",
       " 738: 'compris',\n",
       " 739: 'six',\n",
       " 740: 'rappelle',\n",
       " 741: 'libre',\n",
       " 742: 'venez',\n",
       " 743: 'face',\n",
       " 744: 'consid',\n",
       " 745: 'aies',\n",
       " 746: 'autoris',\n",
       " 747: 'patron',\n",
       " 748: 'plage',\n",
       " 749: 'discuter',\n",
       " 750: 'ressant',\n",
       " 751: 'court',\n",
       " 752: 'prend',\n",
       " 753: 'avance',\n",
       " 754: 'japon',\n",
       " 755: 'lequel',\n",
       " 756: 'parc',\n",
       " 757: 'rience',\n",
       " 758: 'voici',\n",
       " 759: 'ouvrir',\n",
       " 760: 'poisson',\n",
       " 761: 'restez',\n",
       " 762: 'tenir',\n",
       " 763: 'arme',\n",
       " 764: 'biblioth',\n",
       " 765: 'type',\n",
       " 766: 'pire',\n",
       " 767: 'telle',\n",
       " 768: 'parfois',\n",
       " 769: 'dollars',\n",
       " 770: 'filles',\n",
       " 771: 'pluie',\n",
       " 772: 'cision',\n",
       " 773: 'faute',\n",
       " 774: 'conseil',\n",
       " 775: 'lever',\n",
       " 776: 'nombreuses',\n",
       " 777: 'gagner',\n",
       " 778: 'aviez',\n",
       " 779: 'gentil',\n",
       " 780: 'march',\n",
       " 781: 'vis',\n",
       " 782: 'serez',\n",
       " 783: 'devenu',\n",
       " 784: 'amour',\n",
       " 785: 'rend',\n",
       " 786: 'commen',\n",
       " 787: 'vaut',\n",
       " 788: 'tort',\n",
       " 789: 'donc',\n",
       " 790: 'mourir',\n",
       " 791: 'entra',\n",
       " 792: 'moindre',\n",
       " 793: 'fermer',\n",
       " 794: 'fumer',\n",
       " 795: 'conseils',\n",
       " 796: 'cens',\n",
       " 797: 'situation',\n",
       " 798: 'trange',\n",
       " 799: 'affaires',\n",
       " 800: 'mesure',\n",
       " 801: 'vacances',\n",
       " 802: 'commencer',\n",
       " 803: 'liste',\n",
       " 804: 'forc',\n",
       " 805: 'entr',\n",
       " 806: 'retourner',\n",
       " 807: 'bras',\n",
       " 808: 'an',\n",
       " 809: 'suffisamment',\n",
       " 810: 'rieux',\n",
       " 811: 'sentir',\n",
       " 812: 'dangereux',\n",
       " 813: 'poids',\n",
       " 814: 'tent',\n",
       " 815: 'expliquer',\n",
       " 816: 'avaient',\n",
       " 817: 'ferai',\n",
       " 818: 'dr',\n",
       " 819: 'rivi',\n",
       " 820: 'prit',\n",
       " 821: 'garde',\n",
       " 822: 'neuf',\n",
       " 823: 'parlez',\n",
       " 824: 'souci',\n",
       " 825: 'marche',\n",
       " 826: 'sormais',\n",
       " 827: 'cong',\n",
       " 828: 'dirait',\n",
       " 829: 'certaine',\n",
       " 830: 'lunettes',\n",
       " 831: 'opinion',\n",
       " 832: 'tres',\n",
       " 833: 'but',\n",
       " 834: 'emp',\n",
       " 835: 'excuses',\n",
       " 836: 'amusant',\n",
       " 837: 'suite',\n",
       " 838: 'timent',\n",
       " 839: 'danser',\n",
       " 840: 'promis',\n",
       " 841: 'oncle',\n",
       " 842: 'bizarre',\n",
       " 843: 'crains',\n",
       " 844: 'bateau',\n",
       " 845: 'tranger',\n",
       " 846: 'stupide',\n",
       " 847: 'essayez',\n",
       " 848: 'calme',\n",
       " 849: 'cent',\n",
       " 850: 'dents',\n",
       " 851: 'attendais',\n",
       " 852: 'donnez',\n",
       " 853: 'terminer',\n",
       " 854: 'moyen',\n",
       " 855: 'appareil',\n",
       " 856: 'propre',\n",
       " 857: 'continu',\n",
       " 858: 'su',\n",
       " 859: 'lac',\n",
       " 860: 'semblait',\n",
       " 861: 'visage',\n",
       " 862: 'compter',\n",
       " 863: 'piano',\n",
       " 864: 'ferme',\n",
       " 865: 'utile',\n",
       " 866: 'fonctionne',\n",
       " 867: 'pied',\n",
       " 868: 'occuper',\n",
       " 869: 'tudie',\n",
       " 870: 'car',\n",
       " 871: 'quels',\n",
       " 872: 'mit',\n",
       " 873: 'tour',\n",
       " 874: 'japonais',\n",
       " 875: 'manteau',\n",
       " 876: 'seriez',\n",
       " 877: 'aid',\n",
       " 878: 'week',\n",
       " 879: 'end',\n",
       " 880: 'prison',\n",
       " 881: 'attendu',\n",
       " 882: 'habite',\n",
       " 883: 'amies',\n",
       " 884: 'ves',\n",
       " 885: 'carte',\n",
       " 886: 'salle',\n",
       " 887: 'chante',\n",
       " 888: 'satisfait',\n",
       " 889: 'parviens',\n",
       " 890: 'projet',\n",
       " 891: 'gagn',\n",
       " 892: 'coute',\n",
       " 893: 'aise',\n",
       " 894: 'souhaite',\n",
       " 895: 'aie',\n",
       " 896: 'venue',\n",
       " 897: 'tements',\n",
       " 898: 'triste',\n",
       " 899: 'lis',\n",
       " 900: 'plupart',\n",
       " 901: 'courant',\n",
       " 902: 'banque',\n",
       " 903: 'viendra',\n",
       " 904: 'seras',\n",
       " 905: 'auriez',\n",
       " 906: 'sident',\n",
       " 907: 'ach',\n",
       " 908: 'proposition',\n",
       " 909: 'rendu',\n",
       " 910: 'sommeil',\n",
       " 911: 'plusieurs',\n",
       " 912: 'habitu',\n",
       " 913: 'contente',\n",
       " 914: 'rique',\n",
       " 915: 'pleuvoir',\n",
       " 916: 'couter',\n",
       " 917: 'autour',\n",
       " 918: 'suivre',\n",
       " 919: 'vingt',\n",
       " 920: 'honn',\n",
       " 921: 'blague',\n",
       " 922: 'attrap',\n",
       " 923: 'amus',\n",
       " 924: 'message',\n",
       " 925: 'docteur',\n",
       " 926: 'donna',\n",
       " 927: 'certain',\n",
       " 928: 'suppos',\n",
       " 929: 'mettez',\n",
       " 930: 'dormi',\n",
       " 931: 'demanda',\n",
       " 932: 'tat',\n",
       " 933: 'prudent',\n",
       " 934: 'cas',\n",
       " 935: 'douleur',\n",
       " 936: 'finalement',\n",
       " 937: 'fera',\n",
       " 938: 'galement',\n",
       " 939: 'tir',\n",
       " 940: 'seconde',\n",
       " 941: 'effray',\n",
       " 942: 'ouvert',\n",
       " 943: 'emprunter',\n",
       " 944: 'certaines',\n",
       " 945: 'danger',\n",
       " 946: 'mer',\n",
       " 947: 'jeu',\n",
       " 948: 'quitt',\n",
       " 949: 'impliqu',\n",
       " 950: 'haut',\n",
       " 951: 'oublie',\n",
       " 952: 'pommes',\n",
       " 953: 'voix',\n",
       " 954: 'proche',\n",
       " 955: 'avocat',\n",
       " 956: 'jou',\n",
       " 957: 'tableau',\n",
       " 958: 'marier',\n",
       " 959: 'amoureux',\n",
       " 960: 'ufs',\n",
       " 961: 'esprit',\n",
       " 962: 'sac',\n",
       " 963: 'moiti',\n",
       " 964: 'battre',\n",
       " 965: 'meilleure',\n",
       " 966: 'sucre',\n",
       " 967: 'vide',\n",
       " 968: 'aimeriez',\n",
       " 969: 'femmes',\n",
       " 970: 'endormi',\n",
       " 971: 'connaissance',\n",
       " 972: 'aimait',\n",
       " 973: 'range',\n",
       " 974: 'cider',\n",
       " 975: 'sugg',\n",
       " 976: 'rapide',\n",
       " 977: 'chaise',\n",
       " 978: 'couvert',\n",
       " 979: 'hors',\n",
       " 980: 'risque',\n",
       " 981: 'ts',\n",
       " 982: 'voyez',\n",
       " 983: 'regardez',\n",
       " 984: 'oubliez',\n",
       " 985: 'courir',\n",
       " 986: 'impossible',\n",
       " 987: 'mariage',\n",
       " 988: 'avions',\n",
       " 989: 'inconv',\n",
       " 990: 'rends',\n",
       " 991: 'temp',\n",
       " 992: 'surprise',\n",
       " 993: 'senter',\n",
       " 994: 'sorti',\n",
       " 995: 'chemise',\n",
       " 996: 'chir',\n",
       " 997: 'portes',\n",
       " 998: 'mentir',\n",
       " 999: 'sors',\n",
       " 1000: 'asseoir',\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c15d27af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 8028\n",
      "프랑스어 단어장의 크기 : 10891\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4add9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 34\n",
      "프랑스어 시퀀스의 최대 길이 44\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8e5af56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 8028\n",
      "영어 시퀀스의 최대 길이 34\n",
      "프랑스어 단어장의 크기 : 10891\n",
      "프랑스어 시퀀스의 최대 길이 44\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021111a",
   "metadata": {},
   "source": [
    "* encoder 입력(eng) : encoder_input\n",
    "* decoder 입력( \\<sos\\> + fra) : decorder_input <- teaching forcing\n",
    "* decoder 정답(fra + \\<eos\\> ) : decorder_target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a82ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ word for word in line if word != fra_tokenizer.word_index['<eos>'] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ word for word in line if word != fra_tokenizer.word_index['<sos>'] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc8a2865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2334, 158, 128, 33, 41, 3], [1, 6119, 30, 2125, 60, 43], [1, 6120, 11, 401, 1076, 3006, 74, 4467, 25, 365, 13, 751, 4468, 3]]\n",
      "[[2334, 158, 128, 33, 41, 3, 2], [6119, 30, 2125, 60, 43, 2], [6120, 11, 401, 1076, 3006, 74, 4467, 25, 365, 13, 751, 4468, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "#디코더의 입력과 출력\n",
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234378f0",
   "metadata": {},
   "source": [
    "* padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2763dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 34)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 44)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 44)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d8032fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 98  17   4  28 207   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[   1 2334  158  128   33   41    3    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[2334  158  128   33   41    3    2    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])\n",
    "print(decoder_input[0])\n",
    "print(decoder_target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74650c90",
   "metadata": {},
   "source": [
    "* train set/ validation set 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05a7d630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (30000, 34)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (30000, 44)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (30000, 44)\n",
      "영어 검증데이터의 크기(shape) : (3000, 34)\n",
      "프랑스어 검증 입력데이터의 크기(shape) : (3000, 44)\n",
      "프랑스어 검증 출력데이터의 크기(shape) : (3000, 44)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000 \n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "#print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "#print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "#print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_train))\n",
    "\n",
    "print('영어 검증데이터의 크기(shape) :',np.shape(encoder_input_test))\n",
    "print('프랑스어 검증 입력데이터의 크기(shape) :',np.shape(decoder_input_test))\n",
    "print('프랑스어 검증 출력데이터의 크기(shape) :',np.shape(decoder_target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c43c7",
   "metadata": {},
   "source": [
    "## Step 4. 임베딩 층(Embedding layer) 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0e8ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4726b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8028"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10891"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vocab_size\n",
    "fra_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db77d80c",
   "metadata": {},
   "source": [
    "* 임베딩 벡터의 차원과 LSTM의 은닉 상태의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53614037",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "hidden_units = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f56eb",
   "metadata": {},
   "source": [
    "* Encoder 설계\n",
    "  * Masking 은 토큰인 숫자 0의 경우에는 연산에서 제외하는 역할 수행\n",
    "  * LSTM return_state=True 로 설정( encoder의 내부상태를 decoder에 넘겨줌)\n",
    "  * context vector : encoder_state 를 decoder에 전달하기 위해 저장\n",
    "    * state_h : hidden state \n",
    "    * state_c : cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "158d489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(eng_vocab_size, embedding_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(hidden_units, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05dbafd",
   "metadata": {},
   "source": [
    "* Decoder 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb3dd5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(fra_vocab_size, hidden_units) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45405d5c",
   "metadata": {},
   "source": [
    "## Step 5. 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ade19e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    2055168     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    2788096     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 256)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 256)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 525312      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 10891)  2798987     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,692,875\n",
      "Trainable params: 8,692,875\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델의 입력과 출력을 정의.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f0f9808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 27s 82ms/step - loss: 1.6951 - acc: 0.7980 - val_loss: 1.1394 - val_acc: 0.8298\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 17s 74ms/step - loss: 1.0671 - acc: 0.8354 - val_loss: 1.0468 - val_acc: 0.8366\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.9904 - acc: 0.8419 - val_loss: 0.9863 - val_acc: 0.8427\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 18s 76ms/step - loss: 0.9140 - acc: 0.8512 - val_loss: 0.9113 - val_acc: 0.8540\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.8371 - acc: 0.8604 - val_loss: 0.8509 - val_acc: 0.8610\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 18s 78ms/step - loss: 0.7725 - acc: 0.8676 - val_loss: 0.8009 - val_acc: 0.8665\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.7150 - acc: 0.8744 - val_loss: 0.7640 - val_acc: 0.8710\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.6668 - acc: 0.8795 - val_loss: 0.7316 - val_acc: 0.8747\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.6249 - acc: 0.8841 - val_loss: 0.7065 - val_acc: 0.8783\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.5879 - acc: 0.8882 - val_loss: 0.6856 - val_acc: 0.8806\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.5539 - acc: 0.8923 - val_loss: 0.6672 - val_acc: 0.8830\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.5219 - acc: 0.8964 - val_loss: 0.6521 - val_acc: 0.8857\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.4919 - acc: 0.9004 - val_loss: 0.6398 - val_acc: 0.8876\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.4641 - acc: 0.9040 - val_loss: 0.6288 - val_acc: 0.8891\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.4374 - acc: 0.9076 - val_loss: 0.6196 - val_acc: 0.8909\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.4121 - acc: 0.9113 - val_loss: 0.6117 - val_acc: 0.8921\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.3884 - acc: 0.9150 - val_loss: 0.6068 - val_acc: 0.8934\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.3648 - acc: 0.9191 - val_loss: 0.5993 - val_acc: 0.8948\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.3424 - acc: 0.9230 - val_loss: 0.5961 - val_acc: 0.8959\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.3216 - acc: 0.9273 - val_loss: 0.5930 - val_acc: 0.8962\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.3017 - acc: 0.9311 - val_loss: 0.5906 - val_acc: 0.8979\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.2832 - acc: 0.9353 - val_loss: 0.5925 - val_acc: 0.8984\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.2651 - acc: 0.9389 - val_loss: 0.5898 - val_acc: 0.8988\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.2485 - acc: 0.9426 - val_loss: 0.5895 - val_acc: 0.8993\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.2326 - acc: 0.9462 - val_loss: 0.5907 - val_acc: 0.9003\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.2178 - acc: 0.9496 - val_loss: 0.5927 - val_acc: 0.9004\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.2037 - acc: 0.9527 - val_loss: 0.5954 - val_acc: 0.9009\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.1905 - acc: 0.9558 - val_loss: 0.5972 - val_acc: 0.9015\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.1782 - acc: 0.9588 - val_loss: 0.6003 - val_acc: 0.9018\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.1664 - acc: 0.9617 - val_loss: 0.6033 - val_acc: 0.9014\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.1557 - acc: 0.9643 - val_loss: 0.6091 - val_acc: 0.9019\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.1458 - acc: 0.9665 - val_loss: 0.6113 - val_acc: 0.9019\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.1358 - acc: 0.9691 - val_loss: 0.6179 - val_acc: 0.9023\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.1270 - acc: 0.9712 - val_loss: 0.6239 - val_acc: 0.9014\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.1187 - acc: 0.9732 - val_loss: 0.6266 - val_acc: 0.9017\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.1109 - acc: 0.9750 - val_loss: 0.6329 - val_acc: 0.9027\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 18s 78ms/step - loss: 0.1034 - acc: 0.9769 - val_loss: 0.6386 - val_acc: 0.9024\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 18s 78ms/step - loss: 0.0970 - acc: 0.9785 - val_loss: 0.6447 - val_acc: 0.9026\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0898 - acc: 0.9803 - val_loss: 0.6493 - val_acc: 0.9021\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0836 - acc: 0.9819 - val_loss: 0.6574 - val_acc: 0.9025\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0782 - acc: 0.9831 - val_loss: 0.6648 - val_acc: 0.9018\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0728 - acc: 0.9845 - val_loss: 0.6699 - val_acc: 0.9018\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0681 - acc: 0.9855 - val_loss: 0.6773 - val_acc: 0.9018\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0640 - acc: 0.9865 - val_loss: 0.6822 - val_acc: 0.9021\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0597 - acc: 0.9875 - val_loss: 0.6883 - val_acc: 0.9016\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0556 - acc: 0.9885 - val_loss: 0.6964 - val_acc: 0.9018\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0515 - acc: 0.9896 - val_loss: 0.7028 - val_acc: 0.9018\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0484 - acc: 0.9902 - val_loss: 0.7089 - val_acc: 0.9019\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0445 - acc: 0.9913 - val_loss: 0.7167 - val_acc: 0.9011\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 18s 77ms/step - loss: 0.0442 - acc: 0.9909 - val_loss: 0.7239 - val_acc: 0.9013\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313d321",
   "metadata": {},
   "source": [
    "* 학습 결과에 대한 그래프를 시각화\n",
    "  * train loss / val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b36996e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f96e2d03190>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f96e2d03550>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f96e2d03430>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 검증셋과 학습셋의 오차를 저장\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Validation_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Train_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef8071",
   "metadata": {},
   "source": [
    "* 학습 결과에 대한 그래프를 시각화\n",
    "  * train acc / val acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66052322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f96e2c14d60>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f96e2c261f0>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f96e2c14fa0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 검증셋과 학습셋의 accuracy 저장\n",
    "y_vacc = history.history['val_acc']\n",
    "y_acc = history.history['acc']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vacc, marker='.', c=\"red\", label='Validation_acc')\n",
    "plt.plot(x_len, y_acc, marker='.', c=\"blue\", label='Train_acc')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920705c",
   "metadata": {},
   "source": [
    "## Step 6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380862e6",
   "metadata": {},
   "source": [
    "* 테스트 Encoder 설계  \n",
    "인코더의 입, 출력으로 사용하는 encoder_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31b92ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 256)         2055168   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 525312    \n",
      "=================================================================\n",
      "Total params: 2,580,480\n",
      "Trainable params: 2,580,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 인코더\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed397d9",
   "metadata": {},
   "source": [
    "* 테스트 디코더 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "691caa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    2788096     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[1][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 10891)  2798987     lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,112,395\n",
      "Trainable params: 6,112,395\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계 시작\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_units,))\n",
    "decoder_state_input_c = Input(shape=(hidden_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# 수정된 디코더\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7931df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209606a",
   "metadata": {},
   "source": [
    "* decode_sequence 함수를 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5036a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # 입력으로부터 인코더의 마지막 시점의 상태(은닉 상태, 셀 상태)를 얻음\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # <SOS>에 해당하는 정수 생성\n",
    "  target_seq = np.zeros((1,1))\n",
    "  target_seq[0, 0] = fra2idx['<sos>']\n",
    "\n",
    "  stop_condition = False\n",
    "  decoded_sentence = ''\n",
    "\n",
    "  # stop_condition이 True가 될 때까지 루프 반복\n",
    "  # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "  while not stop_condition:\n",
    "    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "    # 예측 결과를 단어로 변환\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "    # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "    decoded_sentence += ' '+sampled_char\n",
    "\n",
    "    # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "    if sampled_char == '<eos>':\n",
    "       stop_condition = True\n",
    "    if len(decoded_sentence) > max_fra_seq_len:\n",
    "        stop_condition = True\n",
    "        print(len(decoded_sentence))\n",
    "\n",
    "    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "    states_value = [h, c]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dec4f5",
   "metadata": {},
   "source": [
    "* 결과 확인을 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "107161fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0):\n",
    "      sentence = sentence + idx2eng[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0 and encoded_word != fra2idx['<sos>'] and encoded_word != fra2idx['<eos>']):\n",
    "      sentence = sentence + idx2fra[encoded_word] + ' '\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d2282",
   "metadata": {},
   "source": [
    "* 출력 결과 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3cba4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "입력문장 : there was a cold wind blowing from the north . \n",
      "정답문장 : il y avait un vent froid soufflant du nord . \n",
      "번역문장 : il y avait un vent froid soufflant du n\n",
      "--------------------------------------------------\n",
      "입력문장 : how was the fishing ? \n",
      "정답문장 : comment tait la p che ? \n",
      "번역문장 : comment tait la p che ? \n",
      "--------------------------------------------------\n",
      "입력문장 : i beg you . \n",
      "정답문장 : je vous en supplie . \n",
      "번역문장 : je vous en supplie . \n",
      "--------------------------------------------------\n",
      "47\n",
      "입력문장 : i ve never read a novel in french . \n",
      "정답문장 : je n ai jamais lu de roman en fran ais . \n",
      "번역문장 : je n ai jamais lu de roman en fran ais . \n",
      "--------------------------------------------------\n",
      "입력문장 : we ve got to be careful . \n",
      "정답문장 : nous devons tre prudentes . \n",
      "번역문장 : nous devons tre prudentes . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [30, 500, 1000, 3000, 3002]:\n",
    "  input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada5160",
   "metadata": {},
   "source": [
    "* 훈련 데이터로 출력 결과 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c67be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : do you want a salad ? \n",
      "정답문장 : tu veux une salade ? \n",
      "번역문장 : tu veux une salade ? \n",
      "--------------------------------------------------\n",
      "입력문장 : he s about the same age as you are . \n",
      "정답문장 : il a peu pr s le m me ge que toi . \n",
      "번역문장 : il a peu pr s le m me ge que toi . \n",
      "--------------------------------------------------\n",
      "46\n",
      "입력문장 : the company presented him with a gold watch on the day he retired . \n",
      "정답문장 : la soci t lui offrit une montre en or le jour de son d part . \n",
      "번역문장 : la soci t lui offrit une montre en or le\n",
      "--------------------------------------------------\n",
      "46\n",
      "입력문장 : given her inexperience , she has done well . \n",
      "정답문장 : tant donn son inexp rience , elle a bien fait . \n",
      "번역문장 : tant donn son inexp rience , elle a bien\n",
      "--------------------------------------------------\n",
      "47\n",
      "입력문장 : how often do you use your phone ? \n",
      "정답문장 : quelle fr quence utilises tu ton t l phone ? \n",
      "번역문장 : quelle fr quence utilisez vous votre t l \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9883135",
   "metadata": {},
   "source": [
    "* 테스트 데이터로 출력 결과 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a520bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "입력문장 : i wish i d followed tom s advice . \n",
      "정답문장 : j aurais d suivre le conseil de tom . \n",
      "번역문장 : j aimerais que tom a t stupide de jouer . \n",
      "--------------------------------------------------\n",
      "48\n",
      "입력문장 : i realized i couldn t win . \n",
      "정답문장 : je pris conscience que je ne pouvais gagner . \n",
      "번역문장 : je pris conscience que je ne pouvais l emp\n",
      "--------------------------------------------------\n",
      "입력문장 : you need a joystick . \n",
      "정답문장 : vous avez besoin d une manette de jeu . \n",
      "번역문장 : tu as besoin d un endroit . \n",
      "--------------------------------------------------\n",
      "입력문장 : are you up ? \n",
      "정답문장 : es tu lev e ? \n",
      "번역문장 : tes vous lev ? \n",
      "--------------------------------------------------\n",
      "45\n",
      "입력문장 : he must finish his homework today . \n",
      "정답문장 : il doit terminer aujourd hui ses devoirs . \n",
      "번역문장 : il doit finir de faire pour le d jeuner\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696fe3c",
   "metadata": {},
   "source": [
    "## 회고\n",
    "* 출력결과 테스트 함수\n",
    "  * 출력결과 테스트 시에 오류로 인해서, [딥러닝을 이용한 자연어 처리 입문](https://wikidocs.net/86900) 책의 출력 결과 확인을 위한 함수(seq_to_src(input_seq), def seq_to_tar(input_seq) ) 이용해서 완성하였음.\n",
    "* 모든 sample의 길이를 동일하게 하기위해서 0으로 padding 시에, 0을 앞으로 채우면 (padding='pre') 0을 뒤로 채울때(padding='post') 보다도 문장 번역이 제대로 되지 않음. 테스트 데이터 뿐만 아니라, 훈련된 데이터로도 제대로된 번역이 되지 않았음. \n",
    "padding 위치 관련해서 좀 더 많은 테스트를 해보고, 왜 그런지에 대한 이유를 찾기로 함\n",
    "* embeddin layer의 임베딩 차원(embedding_dim)과 LSTM의 units를 변경해가면서, 번역이 잘 되는지 테스트해봄\n",
    "  * embedding_dim, hidden_units = 64, 128, 256, 512 로 테스트 해 본 결과 256일때가 유사한 문장으로 번역됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a0ab653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 21s 64ms/step - loss: 2.1482 - acc: 0.7796 - val_loss: 1.2103 - val_acc: 0.8076\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 1.1506 - acc: 0.8253 - val_loss: 1.0990 - val_acc: 0.8306\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 1.0707 - acc: 0.8330 - val_loss: 1.0457 - val_acc: 0.8361\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 1.0129 - acc: 0.8383 - val_loss: 0.9881 - val_acc: 0.8420\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.9556 - acc: 0.8440 - val_loss: 0.9394 - val_acc: 0.8485\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.9009 - acc: 0.8518 - val_loss: 0.8910 - val_acc: 0.8557\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.8502 - acc: 0.8584 - val_loss: 0.8513 - val_acc: 0.8603\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.8064 - acc: 0.8638 - val_loss: 0.8188 - val_acc: 0.8647\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.7687 - acc: 0.8680 - val_loss: 0.7917 - val_acc: 0.8682\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.7342 - acc: 0.8717 - val_loss: 0.7682 - val_acc: 0.8709\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.7028 - acc: 0.8751 - val_loss: 0.7475 - val_acc: 0.8730\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.6742 - acc: 0.8783 - val_loss: 0.7300 - val_acc: 0.8749\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.6476 - acc: 0.8813 - val_loss: 0.7149 - val_acc: 0.8771\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.6234 - acc: 0.8840 - val_loss: 0.7012 - val_acc: 0.8792\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.6007 - acc: 0.8866 - val_loss: 0.6901 - val_acc: 0.8806\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.5796 - acc: 0.8891 - val_loss: 0.6811 - val_acc: 0.8820\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.5599 - acc: 0.8917 - val_loss: 0.6730 - val_acc: 0.8832\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.5412 - acc: 0.8940 - val_loss: 0.6646 - val_acc: 0.8841\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.5232 - acc: 0.8962 - val_loss: 0.6579 - val_acc: 0.8849\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.5065 - acc: 0.8984 - val_loss: 0.6521 - val_acc: 0.8864\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.4903 - acc: 0.9004 - val_loss: 0.6484 - val_acc: 0.8870\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.4753 - acc: 0.9023 - val_loss: 0.6418 - val_acc: 0.8880\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.4602 - acc: 0.9043 - val_loss: 0.6392 - val_acc: 0.8881\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.4461 - acc: 0.9063 - val_loss: 0.6367 - val_acc: 0.8891\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.4328 - acc: 0.9084 - val_loss: 0.6342 - val_acc: 0.8895\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.4194 - acc: 0.9105 - val_loss: 0.6320 - val_acc: 0.8899\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.4069 - acc: 0.9121 - val_loss: 0.6310 - val_acc: 0.8901\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.3946 - acc: 0.9141 - val_loss: 0.6304 - val_acc: 0.8907\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.3831 - acc: 0.9161 - val_loss: 0.6296 - val_acc: 0.8917\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.3717 - acc: 0.9181 - val_loss: 0.6287 - val_acc: 0.8915\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.3609 - acc: 0.9200 - val_loss: 0.6280 - val_acc: 0.8924\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.3503 - acc: 0.9219 - val_loss: 0.6290 - val_acc: 0.8926\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.3399 - acc: 0.9238 - val_loss: 0.6297 - val_acc: 0.8926\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.3302 - acc: 0.9258 - val_loss: 0.6306 - val_acc: 0.8927\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.3208 - acc: 0.9275 - val_loss: 0.6310 - val_acc: 0.8926\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.3112 - acc: 0.9296 - val_loss: 0.6319 - val_acc: 0.8936\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.3022 - acc: 0.9314 - val_loss: 0.6334 - val_acc: 0.8928\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.2935 - acc: 0.9332 - val_loss: 0.6354 - val_acc: 0.8932\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.2852 - acc: 0.9348 - val_loss: 0.6389 - val_acc: 0.8936\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.2772 - acc: 0.9366 - val_loss: 0.6397 - val_acc: 0.8934\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.2697 - acc: 0.9382 - val_loss: 0.6432 - val_acc: 0.8940\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.2619 - acc: 0.9399 - val_loss: 0.6435 - val_acc: 0.8943\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.2546 - acc: 0.9414 - val_loss: 0.6465 - val_acc: 0.8939\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.2478 - acc: 0.9430 - val_loss: 0.6488 - val_acc: 0.8938\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.2406 - acc: 0.9447 - val_loss: 0.6525 - val_acc: 0.8932\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.2338 - acc: 0.9461 - val_loss: 0.6545 - val_acc: 0.8939\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.2282 - acc: 0.9472 - val_loss: 0.6569 - val_acc: 0.8939\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.2218 - acc: 0.9488 - val_loss: 0.6606 - val_acc: 0.8933\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 13s 57ms/step - loss: 0.2155 - acc: 0.9502 - val_loss: 0.6640 - val_acc: 0.8939\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 13s 56ms/step - loss: 0.2102 - acc: 0.9513 - val_loss: 0.6656 - val_acc: 0.8943\n"
     ]
    }
   ],
   "source": [
    "#embedding_dim = 128\n",
    "#hidden_units = 128\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ba7d1785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 26s 83ms/step - loss: 1.7212 - acc: 0.7937 - val_loss: 1.1359 - val_acc: 0.8285\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 18s 76ms/step - loss: 1.0877 - acc: 0.8317 - val_loss: 1.0439 - val_acc: 0.8357\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 18s 76ms/step - loss: 1.0056 - acc: 0.8390 - val_loss: 0.9699 - val_acc: 0.8451\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.9245 - acc: 0.8490 - val_loss: 0.8980 - val_acc: 0.8548\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.8463 - acc: 0.8591 - val_loss: 0.8368 - val_acc: 0.8622\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.7832 - acc: 0.8658 - val_loss: 0.7910 - val_acc: 0.8679\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.7301 - acc: 0.8714 - val_loss: 0.7545 - val_acc: 0.8716\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.6838 - acc: 0.8758 - val_loss: 0.7245 - val_acc: 0.8751\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.6436 - acc: 0.8802 - val_loss: 0.7010 - val_acc: 0.8781\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.6076 - acc: 0.8844 - val_loss: 0.6816 - val_acc: 0.8810\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.5743 - acc: 0.8884 - val_loss: 0.6659 - val_acc: 0.8828\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.5441 - acc: 0.8920 - val_loss: 0.6501 - val_acc: 0.8852\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 18s 76ms/step - loss: 0.5151 - acc: 0.8958 - val_loss: 0.6387 - val_acc: 0.8871\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.4878 - acc: 0.8993 - val_loss: 0.6285 - val_acc: 0.8889\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.4620 - acc: 0.9030 - val_loss: 0.6194 - val_acc: 0.8905\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.4373 - acc: 0.9065 - val_loss: 0.6122 - val_acc: 0.8921\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.4139 - acc: 0.9102 - val_loss: 0.6067 - val_acc: 0.8929\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.3915 - acc: 0.9139 - val_loss: 0.6008 - val_acc: 0.8947\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.3700 - acc: 0.9178 - val_loss: 0.5938 - val_acc: 0.8959\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.3489 - acc: 0.9217 - val_loss: 0.5917 - val_acc: 0.8972\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.3293 - acc: 0.9255 - val_loss: 0.5887 - val_acc: 0.8974\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.3103 - acc: 0.9294 - val_loss: 0.5857 - val_acc: 0.8989\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.2923 - acc: 0.9332 - val_loss: 0.5847 - val_acc: 0.8995\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.2752 - acc: 0.9367 - val_loss: 0.5842 - val_acc: 0.8998\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.2589 - acc: 0.9405 - val_loss: 0.5849 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.2438 - acc: 0.9437 - val_loss: 0.5861 - val_acc: 0.9010\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.2285 - acc: 0.9472 - val_loss: 0.5877 - val_acc: 0.9008\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.2152 - acc: 0.9501 - val_loss: 0.5890 - val_acc: 0.9015\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.2016 - acc: 0.9535 - val_loss: 0.5940 - val_acc: 0.9014\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 18s 76ms/step - loss: 0.1895 - acc: 0.9563 - val_loss: 0.5951 - val_acc: 0.9017\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.1776 - acc: 0.9590 - val_loss: 0.5989 - val_acc: 0.9016\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.1666 - acc: 0.9616 - val_loss: 0.6040 - val_acc: 0.9023\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 18s 76ms/step - loss: 0.1562 - acc: 0.9641 - val_loss: 0.6083 - val_acc: 0.9028\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.1462 - acc: 0.9665 - val_loss: 0.6103 - val_acc: 0.9022\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.1372 - acc: 0.9687 - val_loss: 0.6171 - val_acc: 0.9022\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.1289 - acc: 0.9707 - val_loss: 0.6215 - val_acc: 0.9027\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.1203 - acc: 0.9727 - val_loss: 0.6263 - val_acc: 0.9023\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.1129 - acc: 0.9747 - val_loss: 0.6333 - val_acc: 0.9026\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.1057 - acc: 0.9764 - val_loss: 0.6383 - val_acc: 0.9030\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.0987 - acc: 0.9780 - val_loss: 0.6440 - val_acc: 0.9021\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.0926 - acc: 0.9796 - val_loss: 0.6490 - val_acc: 0.9029\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.0869 - acc: 0.9810 - val_loss: 0.6545 - val_acc: 0.9027\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.0813 - acc: 0.9823 - val_loss: 0.6623 - val_acc: 0.9022\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.0763 - acc: 0.9835 - val_loss: 0.6693 - val_acc: 0.9025\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.0717 - acc: 0.9846 - val_loss: 0.6755 - val_acc: 0.9027\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.0667 - acc: 0.9859 - val_loss: 0.6824 - val_acc: 0.9023\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 18s 76ms/step - loss: 0.0621 - acc: 0.9870 - val_loss: 0.6881 - val_acc: 0.9025\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.0582 - acc: 0.9880 - val_loss: 0.6942 - val_acc: 0.9020\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 18s 75ms/step - loss: 0.0544 - acc: 0.9889 - val_loss: 0.7040 - val_acc: 0.9022\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 18s 76ms/step - loss: 0.0513 - acc: 0.9896 - val_loss: 0.7071 - val_acc: 0.9020\n"
     ]
    }
   ],
   "source": [
    "#embedding_dim = 256\n",
    "#hidden_units = 256\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f418c2f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 37s 131ms/step - loss: 1.4582 - acc: 0.8042 - val_loss: 1.0816 - val_acc: 0.8327\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 1.0297 - acc: 0.8367 - val_loss: 0.9862 - val_acc: 0.8420\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.9355 - acc: 0.8470 - val_loss: 0.9054 - val_acc: 0.8527\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 30s 126ms/step - loss: 0.8540 - acc: 0.8555 - val_loss: 0.8460 - val_acc: 0.8582\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.7854 - acc: 0.8613 - val_loss: 0.7989 - val_acc: 0.8626\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.7286 - acc: 0.8667 - val_loss: 0.7653 - val_acc: 0.8668\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.7497 - acc: 0.8654 - val_loss: 0.7587 - val_acc: 0.8677\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.6612 - acc: 0.8735 - val_loss: 0.7345 - val_acc: 0.8704\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.6267 - acc: 0.8770 - val_loss: 0.7205 - val_acc: 0.8725\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.5953 - acc: 0.8803 - val_loss: 0.7074 - val_acc: 0.8747\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.5675 - acc: 0.8836 - val_loss: 0.6988 - val_acc: 0.8764\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.5398 - acc: 0.8873 - val_loss: 0.6897 - val_acc: 0.8786\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.5147 - acc: 0.8909 - val_loss: 0.6836 - val_acc: 0.8804\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.4910 - acc: 0.8947 - val_loss: 0.6787 - val_acc: 0.8817\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.4689 - acc: 0.8980 - val_loss: 0.6762 - val_acc: 0.8828\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.4481 - acc: 0.9016 - val_loss: 0.6738 - val_acc: 0.8843\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.4287 - acc: 0.9050 - val_loss: 0.6721 - val_acc: 0.8843\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.4098 - acc: 0.9085 - val_loss: 0.6690 - val_acc: 0.8856\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.3907 - acc: 0.9121 - val_loss: 0.6696 - val_acc: 0.8867\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.3731 - acc: 0.9153 - val_loss: 0.6670 - val_acc: 0.8870\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.3559 - acc: 0.9188 - val_loss: 0.6670 - val_acc: 0.8879\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.3389 - acc: 0.9221 - val_loss: 0.6681 - val_acc: 0.8888\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.3234 - acc: 0.9253 - val_loss: 0.6695 - val_acc: 0.8900\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.3083 - acc: 0.9284 - val_loss: 0.6697 - val_acc: 0.8906\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.2930 - acc: 0.9313 - val_loss: 0.6705 - val_acc: 0.8913\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.2779 - acc: 0.9348 - val_loss: 0.6730 - val_acc: 0.8914\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.2647 - acc: 0.9372 - val_loss: 0.6757 - val_acc: 0.8920\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.2512 - acc: 0.9402 - val_loss: 0.6823 - val_acc: 0.8924\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.2379 - acc: 0.9432 - val_loss: 0.6810 - val_acc: 0.8929\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.2252 - acc: 0.9460 - val_loss: 0.6866 - val_acc: 0.8936\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.2126 - acc: 0.9490 - val_loss: 0.6891 - val_acc: 0.8934\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.2009 - acc: 0.9515 - val_loss: 0.6966 - val_acc: 0.8941\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1896 - acc: 0.9542 - val_loss: 0.6989 - val_acc: 0.8941\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1786 - acc: 0.9568 - val_loss: 0.7024 - val_acc: 0.8943\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1684 - acc: 0.9593 - val_loss: 0.7096 - val_acc: 0.8945\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1581 - acc: 0.9618 - val_loss: 0.7155 - val_acc: 0.8949\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1481 - acc: 0.9643 - val_loss: 0.7225 - val_acc: 0.8942\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1398 - acc: 0.9664 - val_loss: 0.7264 - val_acc: 0.8953\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1310 - acc: 0.9686 - val_loss: 0.7328 - val_acc: 0.8949\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1228 - acc: 0.9709 - val_loss: 0.7415 - val_acc: 0.8940\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1151 - acc: 0.9729 - val_loss: 0.7474 - val_acc: 0.8949\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1071 - acc: 0.9750 - val_loss: 0.7539 - val_acc: 0.8954\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.1003 - acc: 0.9767 - val_loss: 0.7609 - val_acc: 0.8952\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.0937 - acc: 0.9783 - val_loss: 0.7715 - val_acc: 0.8955\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.0874 - acc: 0.9801 - val_loss: 0.7752 - val_acc: 0.8950\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.0811 - acc: 0.9818 - val_loss: 0.7827 - val_acc: 0.8949\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.0759 - acc: 0.9831 - val_loss: 0.7898 - val_acc: 0.8954\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.0706 - acc: 0.9843 - val_loss: 0.7986 - val_acc: 0.8952\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.0654 - acc: 0.9858 - val_loss: 0.8050 - val_acc: 0.8949\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 30s 127ms/step - loss: 0.0609 - acc: 0.9869 - val_loss: 0.8143 - val_acc: 0.8950\n"
     ]
    }
   ],
   "source": [
    "#embedding_dim = 512\n",
    "#hidden_units = 512\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ccc72",
   "metadata": {},
   "source": [
    "* 훈련데이터로 출력 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3686d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "입력문장 : i thought you d be more helpful . \n",
      "정답문장 : je pensais que tu serais plus serviable . \n",
      "번역문장 : je pensais que vous seriez plus heureux . \n",
      "--------------------------------------------------\n",
      "입력문장 : i walk in the forest every day . \n",
      "정답문장 : je marche dans la for t tous les jours . \n",
      "번역문장 : je suis la salle de la banque . \n",
      "--------------------------------------------------\n",
      "45\n",
      "입력문장 : no one is going to hurt you . \n",
      "정답문장 : personne ne va vous faire de mal . \n",
      "번역문장 : personne ne peut te faire une lettre . \n",
      "--------------------------------------------------\n",
      "입력문장 : no one is coming with me . \n",
      "정답문장 : personne ne vient avec moi . \n",
      "번역문장 : personne ne vient avec moi . \n",
      "--------------------------------------------------\n",
      "입력문장 : add and , and you get . \n",
      "정답문장 : cinq plus deux galent sept . \n",
      "번역문장 : aujourd hui , il est tremp e . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#embedding_dim = 128\n",
    "#hidden_units = 128\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3daffbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "입력문장 : i thought you d be more helpful . \n",
      "정답문장 : je pensais que tu serais plus serviable . \n",
      "번역문장 : je pensais que vous seriez plus serviab\n",
      "--------------------------------------------------\n",
      "48\n",
      "입력문장 : i walk in the forest every day . \n",
      "정답문장 : je marche dans la for t tous les jours . \n",
      "번역문장 : je travaille sur le contr le jour tous les\n",
      "--------------------------------------------------\n",
      "입력문장 : no one is going to hurt you . \n",
      "정답문장 : personne ne va vous faire de mal . \n",
      "번역문장 : personne ne va vous faire de mal . \n",
      "--------------------------------------------------\n",
      "입력문장 : no one is coming with me . \n",
      "정답문장 : personne ne vient avec moi . \n",
      "번역문장 : personne ne vient avec moi . \n",
      "--------------------------------------------------\n",
      "입력문장 : add and , and you get . \n",
      "정답문장 : cinq plus deux galent sept . \n",
      "번역문장 : cinq plus deux galent sept . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#embedding_dim = 256\n",
    "#hidden_units = 256\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "05abd0c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "입력문장 : i thought you d be more helpful . \n",
      "정답문장 : je pensais que tu serais plus serviable . \n",
      "번역문장 : je pensais que tu serais plus serviable . \n",
      "--------------------------------------------------\n",
      "입력문장 : i walk in the forest every day . \n",
      "정답문장 : je marche dans la for t tous les jours . \n",
      "번역문장 : je l ai lu sous le lit . \n",
      "--------------------------------------------------\n",
      "입력문장 : no one is going to hurt you . \n",
      "정답문장 : personne ne va vous faire de mal . \n",
      "번역문장 : personne ne savait pourquoi . \n",
      "--------------------------------------------------\n",
      "입력문장 : no one is coming with me . \n",
      "정답문장 : personne ne vient avec moi . \n",
      "번역문장 : personne ne m a soutenu . \n",
      "--------------------------------------------------\n",
      "입력문장 : add and , and you get . \n",
      "정답문장 : cinq plus deux galent sept . \n",
      "번역문장 : montez , tom n aime pas . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#embedding_dim = 512\n",
    "#hidden_units = 512\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e5850",
   "metadata": {},
   "source": [
    "* 테스트 데이티로 출력 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3dcedd8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "입력문장 : why don t you ride your bicycle to work ? \n",
      "정답문장 : pourquoi ne vas tu pas au travail v lo ? \n",
      "번역문장 : pourquoi ne me fais tu pas ton train de \n",
      "--------------------------------------------------\n",
      "45\n",
      "입력문장 : where s the nearest post office ? \n",
      "정답문장 : o est le bureau de poste le plus proche ? \n",
      "번역문장 : le film tait pr sident du temps en aust\n",
      "--------------------------------------------------\n",
      "45\n",
      "입력문장 : the garden can t be seen from the outside . \n",
      "정답문장 : le jardin n est pas visible de l ext rieur . \n",
      "번역문장 : la derni re ne jamais s r qu il y a pas\n",
      "--------------------------------------------------\n",
      "입력문장 : i ll be better in a minute . \n",
      "정답문장 : j irai mieux dans une minute . \n",
      "번역문장 : je serai en retard . \n",
      "--------------------------------------------------\n",
      "47\n",
      "입력문장 : i don t have time to explain it now . \n",
      "정답문장 : je n ai pas le temps de l expliquer pour le moment . \n",
      "번역문장 : je n ai pas de temps pour moi en mieux . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#embedding_dim = 128\n",
    "#hidden_units = 128\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "780cbd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "입력문장 : why don t you ride your bicycle to work ? \n",
      "정답문장 : pourquoi ne vas tu pas au travail v lo ? \n",
      "번역문장 : pourquoi est tu pas pour ton avis , qui \n",
      "--------------------------------------------------\n",
      "49\n",
      "입력문장 : where s the nearest post office ? \n",
      "정답문장 : o est le bureau de poste le plus proche ? \n",
      "번역문장 : o se trouve le bureau deux le plus proche b\n",
      "--------------------------------------------------\n",
      "45\n",
      "입력문장 : the garden can t be seen from the outside . \n",
      "정답문장 : le jardin n est pas visible de l ext rieur . \n",
      "번역문장 : les gens ne peuvent pas voir dans le tr\n",
      "--------------------------------------------------\n",
      "49\n",
      "입력문장 : i ll be better in a minute . \n",
      "정답문장 : j irai mieux dans une minute . \n",
      "번역문장 : je vais vous y retourner pour une minute . \n",
      "--------------------------------------------------\n",
      "입력문장 : i don t have time to explain it now . \n",
      "정답문장 : je n ai pas le temps de l expliquer pour le moment . \n",
      "번역문장 : je n ai pas le temps maintenant . \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#embedding_dim = 256\n",
    "#hidden_units = 256\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7dbb9d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "입력문장 : why don t you ride your bicycle to work ? \n",
      "정답문장 : pourquoi ne vas tu pas au travail v lo ? \n",
      "번역문장 : pourquoi ne commencez vous pas simplement\n",
      "--------------------------------------------------\n",
      "44\n",
      "입력문장 : where s the nearest post office ? \n",
      "정답문장 : o est le bureau de poste le plus proche ? \n",
      "번역문장 : o se trouve le magasin le plus proche \n",
      "--------------------------------------------------\n",
      "51\n",
      "입력문장 : the garden can t be seen from the outside . \n",
      "정답문장 : le jardin n est pas visible de l ext rieur . \n",
      "번역문장 : les choses ne furent pas regard la nuit le pr\n",
      "--------------------------------------------------\n",
      "45\n",
      "입력문장 : i ll be better in a minute . \n",
      "정답문장 : j irai mieux dans une minute . \n",
      "번역문장 : je serai de retour en australie apr s d\n",
      "--------------------------------------------------\n",
      "47\n",
      "입력문장 : i don t have time to explain it now . \n",
      "정답문장 : je n ai pas le temps de l expliquer pour le moment . \n",
      "번역문장 : je n en ai pas le temps pour faire des co\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#embedding_dim = 512\n",
    "#hidden_units = 512\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "  print(\"번역문장 :\",decoded_sentence[1:-5])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7483d5",
   "metadata": {},
   "source": [
    "## 참고 문헌\n",
    "* [딥러닝을 이용한 자연어 처리 입문](https://wikidocs.net/86900)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
