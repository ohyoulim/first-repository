{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7cc3203",
   "metadata": {},
   "source": [
    "# 루브릭 \n",
    "- klue/bert-base를 pretrained weight 없이 사용해, 모델이 정상적으로 작동하는 것을 확인해보았다.\n",
    "\n",
    "- 적절한 전처리와 fine-tuning을 통해 val_acc를 90% 이상 달성했다.\t\n",
    "\n",
    "- 클래스 분포를 살펴보고 Bucketing을 수행하여 어떤 효과를 기대할 수 있는지 예상한 결과를 기록했다.\n",
    "\n",
    "- Bucketing을 수행하여 finetuning 시 연산속도와 모델성능간의 trade-off관계가 발생하는지 여부를 확인하고 분석한 결과를 제시하였다.\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9831d",
   "metadata": {},
   "source": [
    "### dataset\n",
    "1. [Naver sentiment movie corpus v1.0](https://github.com/e9t/nsmc/)\n",
    "\n",
    "\n",
    "### 적용할 알고리즘\n",
    "1. [korBert](https://huggingface.co/klue/bert-base)\n",
    "\n",
    "\n",
    "### 성능 평가 지표  \n",
    "1. accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80bc0a",
   "metadata": {},
   "source": [
    "## 문제 정의  \n",
    "\n",
    " > [klue/bert-base](https://huggingface.co/klue/bert-base)를 사용하여 [NSMC task](https://github.com/e9t/nsmc/)를 수행하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ea284",
   "metadata": {},
   "source": [
    "## 라이브러리 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c46fe1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.27.1\n",
      "Uninstalling transformers-4.27.1:\n",
      "  Successfully uninstalled transformers-4.27.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.27.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.26.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.21.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.3.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.13.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.10.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2021.11.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.12.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->evaluate) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.9/site-packages (4.8.3)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (3.19.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (2.26.0)\n",
      "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (1.12.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (4.62.3)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (0.12.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (1.21.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (5.8.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (1.5.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (0.1.8)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (8.0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.9/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.0.1)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.9/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.6.0)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.9/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (5.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from absl-py->tensorflow-datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall transformers -y\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install tensorflow-datasets -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e0981a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c70fc4ee6e4d089c87c9c2fb1fc875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9978194236755371}]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', framework='tf')\n",
    "classifier('We are very happy to include pipeline into the transformers repository.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4cb0d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "98f737a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.27.1\n",
      "1.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import argparse\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(argparse.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "90af8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7c2045a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt, Mecab\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b2b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b89fe",
   "metadata": {},
   "source": [
    "## STEP 1. nsmc 데이터셋 로드 및 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222bb5e",
   "metadata": {},
   "source": [
    "### huggingface datasets 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde93dee",
   "metadata": {},
   "source": [
    "* dataset 확인\n",
    "  1. train dataset\n",
    "     * id, document, label\n",
    "  2. test dataset \n",
    "     * id, document, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "873cfb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a4256cdbdb4478aceae5734c493ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_nsmc_dataset = load_dataset('nsmc')\n",
    "print(huggingface_nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "203f566c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아 더빙.. 진짜 짜증나네요 목소리',\n",
       " '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나',\n",
       " '너무재밓었다그래서보는것을추천한다',\n",
       " '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정',\n",
       " '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_nsmc_dataset['train']['document'][:5]\n",
    "huggingface_nsmc_dataset['train']['label'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73256cff",
   "metadata": {},
   "source": [
    "## STEP 2. tokenizer와 model 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477f248",
   "metadata": {},
   "source": [
    "### Huggingface Auto Classes를 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c427a",
   "metadata": {},
   "source": [
    "* AutoModelForSequenceClassification 클래스 이용하여 Pretrained model 로딩\n",
    "* Warning  \n",
    "  Text Classification task에 적합한 model head 추가됨  \n",
    "  새로운 head에 대한 가중치가 무작위로 초기화 됨  \n",
    "  그래서 추가한 task에 대해서는 train을 해야 한다고 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "06951849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels = 2)\n",
    "#huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', output_hidden_states = True,num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "76e9aa94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='klue/bert-base', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "910240d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertForSequenceClassification"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.configuration_bert.BertConfig"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_model.__class__\n",
    "huggingface_model.config.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "81e7db37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"klue/bert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.27.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c6406",
   "metadata": {},
   "source": [
    "## STEP 3. tokenizer를 활용하여 데이터셋 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0a595",
   "metadata": {},
   "source": [
    "* transformer func  \n",
    "  * 데이터셋의 개별 항목이 담겨진 딕셔너리를 매개변수로 입력받아서 input_ids, attention_mask 및 token_type_ids 키가 지정된 새로운 딕셔너리를 반환\n",
    "  * return_token_type_ids = False : token_type_ids 미반환 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3fc6ac",
   "metadata": {},
   "source": [
    "#### 1. max_length or longest로 padding 처리 시 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6a13d81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['9976970', '3819312', '10265843', '9045019', '6483659'], 'document': ['아 더빙.. 진짜 짜증나네요 목소리', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다'], 'label': [0, 1, 0, 0, 1]}\n",
      "\n",
      "{'input_ids': [[2, 1376, 831, 2604, 18, 18, 4229, 9801, 2075, 2203, 2182, 4243, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 1963, 18, 18, 18, 11811, 2178, 2088, 28883, 16516, 2776, 18, 18, 18, 18, 10737, 2156, 2015, 2446, 2232, 6758, 2118, 1380, 6074, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 12381, 3758, 2251, 2615, 18, 18, 7946, 4697, 2259, 1415, 2062, 18, 18, 20609, 4221, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 3734, 2582, 2743, 2029, 2079, 26268, 2255, 2957, 4483, 2116, 21486, 2414, 3771, 5, 29493, 27135, 795, 15882, 2052, 2015, 2154, 1902, 2414, 21898, 3113, 28470, 2265, 2116, 5699, 2119, 21690, 2178, 2507, 2062, 3]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "def transform(data):\n",
    "  return huggingface_tokenizer(\n",
    "      data['document'],\n",
    "      truncation = True,\n",
    "      #padding = 'max_length',\n",
    "      padding = 'longest',\n",
    "      return_token_type_ids = False,\n",
    "      )\n",
    "\n",
    "\n",
    "examples = huggingface_nsmc_dataset['train'][:5]\n",
    "examples_transformed = transform(examples)\n",
    "\n",
    "print(examples)\n",
    "print()\n",
    "print(examples_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b5342a",
   "metadata": {},
   "source": [
    "#### 2. dynamic padding 처리 시 \n",
    "* padding 생략\n",
    "* 모든 data를 최대길이로 padding 하는 것은 비효율적임\n",
    "* 배치(batch) 형태로 실행할 때 data에 채우는 것(padding)이 효과적임.\n",
    "  * 전체 데이터셋에서의 최대 길이가 아니라 해당 배치(batch) 내에서의 최대 길이로 채우기(padding) 처리.\n",
    "  * 입력의 길이가 매우 가변적일 때 많은 시간과 처리 능력을 절약!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "82d13138",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['6270596', '9274899', '8544678', '6825595', '6723715'], 'document': ['굳 ㅋ', 'GDNTOPCLASSINTHECLUB', '뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아', '지루하지는 않은데 완전 막장임... 돈주고 보기에는....', '3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??'], 'label': [1, 0, 0, 0, 0]}\n",
      "\n",
      "{'input_ids': [[2, 618, 191, 3], [2, 43, 2134, 2111, 11216, 2325, 2108, 21072, 2238, 14948, 2111, 2081, 2109, 10990, 2237, 2309, 2206, 3], [2, 1097, 2275, 1504, 20609, 2031, 2073, 18, 18, 18, 18, 8170, 2043, 1380, 3683, 3633, 2532, 5708, 2259, 14236, 3614, 9958, 3], [2, 9734, 2205, 2118, 2259, 1380, 2073, 2147, 5124, 22657, 2289, 18, 18, 18, 850, 2223, 2088, 1160, 12551, 2259, 18, 18, 18, 18, 3], [2, 23, 2134, 2154, 3614, 2359, 6186, 1156, 5437, 558, 1572, 2069, 2436, 2147, 18, 18, 1460, 23, 2134, 2200, 8678, 1545, 17874, 2138, 5153, 2205, 2318, 1889, 2321, 35, 35, 3]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "def transform(data):\n",
    "  return huggingface_tokenizer(\n",
    "      data['document'],\n",
    "      truncation = True,\n",
    "#      padding = 'max_length',\n",
    "      return_token_type_ids = False,\n",
    "      )\n",
    "\n",
    "\n",
    "examples = huggingface_nsmc_dataset['test'][:5]\n",
    "examples_transformed = transform(examples)\n",
    "\n",
    "print(examples)\n",
    "print()\n",
    "print(examples_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b0ec7",
   "metadata": {},
   "source": [
    "#### map 을 이용하여 tokenization 진행\n",
    "* 특정 데이터를 dataset 객체로 유지하기 위해 Dataset.map() 메서드를 사용\n",
    "* batched=True를 사용\n",
    "  * 함수가 각 요소에 개별적으로 적용되지 않고 데이터셋의 하부집합, 즉 각 배치(batch) 내에 존재하는 모든 요소들에 한꺼번에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0c6000f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-0bd50172299e8e6d.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-9b598f8623c74919.arrow\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = huggingface_nsmc_dataset.map(transform, batched=True)\n",
    "\n",
    "#encoded_dataset = encoded_dataset.remove_columns(['id','document'])\n",
    "#encoded_dataset = encoded_dataset.remove_columns(['label'])\n",
    "#encoded_dataset = encoded_dataset.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f382ba18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5547cfb9",
   "metadata": {},
   "source": [
    "#### 1. max_length or longest로 padding 처리 시 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ae4aa20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 고정패팅\n",
    "len(encoded_dataset['train'][3]['input_ids'])\n",
    "len(encoded_dataset['test'][3]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05becc",
   "metadata": {},
   "source": [
    "#### 2. dynamic padding 처리 시 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b99e3c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 가변 패팅\n",
    "len(encoded_dataset['train'][3]['input_ids'])\n",
    "len(encoded_dataset['test'][3]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6973f",
   "metadata": {},
   "source": [
    "#### 문장 길이 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "db8fb185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 127,\n",
       " 134,\n",
       " 142}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 122}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = encoded_dataset[\"train\"][:]\n",
    "train = {k: v for k, v in train.items() if k not in [\"id\", \"document\"]}\n",
    "set([len(x) for x in train[\"input_ids\"]])\n",
    "\n",
    "test = encoded_dataset[\"test\"][:]\n",
    "test = {k: v for k, v in test.items() if k not in [\"id\", \"document\"]}\n",
    "set([len(x) for x in test[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc190d",
   "metadata": {},
   "source": [
    "#### 동적 패딩(Dynamic padding)\n",
    " * 전체 요소들을 배치(batch)로 분리할 때 가장 긴 요소의 길이로 모든 예제를 채우는(padding)\n",
    " * 전체 데이터셋이 아닌 개별 배치(batch)에 대해서 별도로 패딩(padding)을 수행하여 과도하게 긴 입력으로 인한 과도한 패딩(padding) 작업을 방지하기 위함\n",
    " * 샘플들을 함께 모아서 지정된 크기의 배치(batch)로 구성하는 역할을 하는 함수를 콜레이트 함수(collate function)\n",
    " * DataCollatorWithPadding  \n",
    "   배치(batch)로 분리하려는 데이터셋의 요소 각각에 대해서 정확한 수의 패딩(padding)을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2f8e4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=huggingface_tokenizer)\n",
    "#data_collator = DataCollatorWithPadding(tokenizer=huggingface_tokenizer,return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d4273",
   "metadata": {},
   "source": [
    "#### data_collator test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e5dfddbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 25,\n",
       " 3,\n",
       " 17,\n",
       " 36,\n",
       " 29,\n",
       " 14,\n",
       " 61,\n",
       " 15,\n",
       " 31,\n",
       " 15,\n",
       " 32,\n",
       " 21,\n",
       " 30,\n",
       " 32,\n",
       " 12,\n",
       " 39,\n",
       " 18,\n",
       " 26,\n",
       " 21,\n",
       " 23,\n",
       " 13,\n",
       " 69,\n",
       " 16,\n",
       " 12,\n",
       " 33,\n",
       " 14,\n",
       " 9,\n",
       " 6,\n",
       " 26,\n",
       " 29,\n",
       " 13]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = encoded_dataset[\"train\"][:32]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"id\", \"document\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "adf6d0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([32, 69]),\n",
       " 'attention_mask': torch.Size([32, 69]),\n",
       " 'labels': torch.Size([32])}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736b9c4",
   "metadata": {},
   "source": [
    "## STEP 4. Train 및 Evaluation 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7939b31b",
   "metadata": {},
   "source": [
    "### Trainer Class를 활용한 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2086e895",
   "metadata": {},
   "source": [
    "#### 1. max_length or longest로 padding 처리 시 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cacb4b0",
   "metadata": {},
   "source": [
    "* TrainingArguments를 통해 학습 관련 설정 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b44c01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer을 활용하는 형태로 모델 재생성\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = './data'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir, # output이 저장될 경로\n",
    "    evaluation_strategy=\"steps\", #evaluation하는 빈도\n",
    "    learning_rate = 2e-5, #learning_rate\n",
    "    per_device_train_batch_size = 32, # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 32, # evaluation 시에 batch size\n",
    "    num_train_epochs = 2, # train 시킬 총 epochs\n",
    "    weight_decay = 0.01, # weight decay\n",
    "#    group_by_length = True,\n",
    "    save_strategy = 'steps'\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2034cd",
   "metadata": {},
   "source": [
    "* compute_metrics 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "66a1d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_metric\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "#metric = load_metric('accuracy')\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6ea72",
   "metadata": {},
   "source": [
    "* Trainer에 model, arguments, train_dataset, eval_dataset, compute_metrics를 넣고 train을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e35c658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,                           # 학습시킬 model\n",
    "    args=training_arguments,                  # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=encoded_dataset['train'],    # training dataset\n",
    "    eval_dataset=encoded_dataset['test'],       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=huggingface_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21319f97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9376' max='9376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9376/9376 3:12:32, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.303762</td>\n",
       "      <td>0.872520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.295090</td>\n",
       "      <td>0.872840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.263416</td>\n",
       "      <td>0.889220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.258611</td>\n",
       "      <td>0.893780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.258808</td>\n",
       "      <td>0.893300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.250900</td>\n",
       "      <td>0.254405</td>\n",
       "      <td>0.893940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.241779</td>\n",
       "      <td>0.900060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.259400</td>\n",
       "      <td>0.238713</td>\n",
       "      <td>0.900740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.236683</td>\n",
       "      <td>0.902020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.246748</td>\n",
       "      <td>0.902040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.177800</td>\n",
       "      <td>0.253234</td>\n",
       "      <td>0.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.258830</td>\n",
       "      <td>0.904840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.256640</td>\n",
       "      <td>0.905240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.256989</td>\n",
       "      <td>0.905040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.174900</td>\n",
       "      <td>0.257122</td>\n",
       "      <td>0.905280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.250178</td>\n",
       "      <td>0.905540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.255090</td>\n",
       "      <td>0.905840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.248414</td>\n",
       "      <td>0.907060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9376, training_loss=0.22494078170724288, metrics={'train_runtime': 11552.9562, 'train_samples_per_second': 25.967, 'train_steps_per_second': 0.812, 'total_flos': 1.583527308907776e+16, 'train_loss': 0.22494078170724288, 'epoch': 2.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f0bd2",
   "metadata": {},
   "source": [
    "* evaluation 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "58ed593c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_dataset['test'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba7f5aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24729055166244507,\n",
       " 'eval_accuracy': 0.90762,\n",
       " 'eval_runtime': 340.5475,\n",
       " 'eval_samples_per_second': 146.822,\n",
       " 'eval_steps_per_second': 4.59,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(encoded_dataset['test'])\n",
    "\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933c9d94",
   "metadata": {},
   "source": [
    "*  predict 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5bb81367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.1016142 ,  2.5430646 ],\n",
       "       [-0.55479914,  1.1290125 ],\n",
       "       [ 1.5495117 , -1.1944097 ],\n",
       "       ...,\n",
       "       [ 0.11867686,  0.37726644],\n",
       "       [ 3.7675648 , -3.4558897 ],\n",
       "       [ 1.9369565 , -1.2857406 ]], dtype=float32), label_ids=array([1, 0, 0, ..., 0, 0, 0]), metrics={'test_loss': 0.24729055166244507, 'test_accuracy': 0.90762, 'test_runtime': 340.3341, 'test_samples_per_second': 146.914, 'test_steps_per_second': 4.593})"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts = trainer.predict(encoded_dataset['test'])\n",
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "03a338a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 2), (50000,))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts.predictions.shape, predicts.label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e16fc53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predicts.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "50d15098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.90762}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from datasets import load_metric\n",
    "import evaluate\n",
    "\n",
    "#metric = load_metric(\"glue\", \"mrpc\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "metric.compute(predictions=preds, references=predicts.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9545aac",
   "metadata": {},
   "source": [
    "* 모델 저장 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0213c70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nsmc_model/bert-base/tokenizer_config.json',\n",
       " 'nsmc_model/bert-base/special_tokens_map.json',\n",
       " 'nsmc_model/bert-base/vocab.txt',\n",
       " 'nsmc_model/bert-base/added_tokens.json',\n",
       " 'nsmc_model/bert-base/tokenizer.json')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_model.save_pretrained('nsmc_model/bert-base')\n",
    "huggingface_tokenizer.save_pretrained('nsmc_model/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9a931623",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('nsmc_model/bert-base')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('nsmc_model/bert-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11450da",
   "metadata": {},
   "source": [
    "#### 2. dynamic padding 처리 시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0900f5c",
   "metadata": {},
   "source": [
    "#### 2.1 TrainingArguments를 통해 학습 관련 설정 지정\n",
    "> group_by_length : False (bucketing 수행)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ba732385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer을 활용하는 형태로 모델 재생성\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = './data'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir, # output이 저장될 경로\n",
    "    evaluation_strategy=\"steps\", #evaluation하는 빈도\n",
    "    learning_rate = 2e-5, #learning_rate\n",
    "    per_device_train_batch_size = 32, # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 32, # evaluation 시에 batch size\n",
    "    num_train_epochs = 2, # train 시킬 총 epochs\n",
    "    weight_decay = 0.01, # weight decay\n",
    "    group_by_length = False,\n",
    "    save_strategy = 'steps'\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151699d",
   "metadata": {},
   "source": [
    "#### 2.2 TrainingArguments를 통해 학습 관련 설정 지정\n",
    "> group_by_length : True (bucketing 수행 - 문장 길이에 따라 같은 길이 끼리 묶어서 할당)  \n",
    "> group_by_length (bool, optional, defaults to False) — Whether or not to group together samples of roughly the same length in the training dataset (to minimize padding applied and be more efficient). Only useful if applying dynamic padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "16d32671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer을 활용하는 형태로 모델 재생성\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = './data'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_d3ir, # output이 저장될 경로\n",
    "    evaluation_strategy=\"steps\", #evaluation하는 빈도\n",
    "    learning_rate = 2e-5, #learning_rate\n",
    "    per_device_train_batch_size = 32, # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 32, # evaluation 시에 batch size\n",
    "    num_train_epochs = 2, # train 시킬 총 epochs\n",
    "    weight_decay = 0.01, # weight decay\n",
    "    group_by_length = True,\n",
    "    save_strategy = 'steps'\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520b585",
   "metadata": {},
   "source": [
    "* compute_metrics 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4e975e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_metric\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "#metric = load_metric('accuracy')\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a9363",
   "metadata": {},
   "source": [
    "* Trainer에 model, arguments, train_dataset, eval_dataset, compute_metrics를 넣고 train을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "26d37587",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,                           # 학습시킬 model\n",
    "    args=training_arguments,                  # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=encoded_dataset['train'],    # training dataset\n",
    "    eval_dataset=encoded_dataset['test'],       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=huggingface_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82c3ec",
   "metadata": {},
   "source": [
    "#### 2.1 group_by_length : False (bucketing 수행)  시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "8a17c117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9376' max='9376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9376/9376 2:22:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>0.319819</td>\n",
       "      <td>0.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.290900</td>\n",
       "      <td>0.276954</td>\n",
       "      <td>0.883520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.268595</td>\n",
       "      <td>0.888980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.300485</td>\n",
       "      <td>0.872540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.263500</td>\n",
       "      <td>0.262609</td>\n",
       "      <td>0.893240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.267795</td>\n",
       "      <td>0.889640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.244261</td>\n",
       "      <td>0.900960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.249537</td>\n",
       "      <td>0.899660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.236889</td>\n",
       "      <td>0.902140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.208100</td>\n",
       "      <td>0.256768</td>\n",
       "      <td>0.904040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.257318</td>\n",
       "      <td>0.902500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.185100</td>\n",
       "      <td>0.252583</td>\n",
       "      <td>0.904260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.257441</td>\n",
       "      <td>0.905140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.181900</td>\n",
       "      <td>0.250150</td>\n",
       "      <td>0.905220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.905480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.253997</td>\n",
       "      <td>0.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.251601</td>\n",
       "      <td>0.906080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>0.245954</td>\n",
       "      <td>0.907500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9376, training_loss=0.22595171798211316, metrics={'train_runtime': 8535.0351, 'train_samples_per_second': 35.149, 'train_steps_per_second': 1.099, 'total_flos': 1.12945353845568e+16, 'train_loss': 0.22595171798211316, 'epoch': 2.0})"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274770b3",
   "metadata": {},
   "source": [
    "#### 2.2 group_by_length : True (bucketing 수행 - 문장 길이에 따라 같은 길이 끼리 묶어서 할당) 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ce3bcfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9376' max='9376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9376/9376 1:43:18, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.342195</td>\n",
       "      <td>0.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>0.277589</td>\n",
       "      <td>0.881180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.283300</td>\n",
       "      <td>0.270469</td>\n",
       "      <td>0.886680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.272900</td>\n",
       "      <td>0.275917</td>\n",
       "      <td>0.885140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.259461</td>\n",
       "      <td>0.891360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>0.247356</td>\n",
       "      <td>0.897920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.250900</td>\n",
       "      <td>0.254483</td>\n",
       "      <td>0.896240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>0.240560</td>\n",
       "      <td>0.901400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.236876</td>\n",
       "      <td>0.901440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.204700</td>\n",
       "      <td>0.258626</td>\n",
       "      <td>0.902220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.272137</td>\n",
       "      <td>0.898760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.184300</td>\n",
       "      <td>0.254732</td>\n",
       "      <td>0.903280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.171700</td>\n",
       "      <td>0.268797</td>\n",
       "      <td>0.903460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.257339</td>\n",
       "      <td>0.904820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.177000</td>\n",
       "      <td>0.251208</td>\n",
       "      <td>0.905560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.252044</td>\n",
       "      <td>0.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.246986</td>\n",
       "      <td>0.906520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.173200</td>\n",
       "      <td>0.246578</td>\n",
       "      <td>0.907100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9376, training_loss=0.22496741786344873, metrics={'train_runtime': 6199.5218, 'train_samples_per_second': 48.391, 'train_steps_per_second': 1.512, 'total_flos': 3617678344554240.0, 'train_loss': 0.22496741786344873, 'epoch': 2.0})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65058d",
   "metadata": {},
   "source": [
    "* evaluation 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a9332ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_dataset['test'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0e3f6964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24855229258537292,\n",
       " 'eval_accuracy': 0.90688,\n",
       " 'eval_runtime': 241.4668,\n",
       " 'eval_samples_per_second': 207.068,\n",
       " 'eval_steps_per_second': 6.473,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(encoded_dataset['test'])\n",
    "\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf2063e",
   "metadata": {},
   "source": [
    "*  predict 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "11b944ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-2.5670576 ,  1.743356  ],\n",
       "       [-0.97058636,  0.56206685],\n",
       "       [ 2.0569272 , -1.8019967 ],\n",
       "       ...,\n",
       "       [ 0.35250664, -0.2036135 ],\n",
       "       [ 3.7146888 , -3.6267927 ],\n",
       "       [ 1.7800817 , -1.7072463 ]], dtype=float32), label_ids=array([1, 0, 0, ..., 0, 0, 0]), metrics={'test_loss': 0.24855229258537292, 'test_accuracy': 0.90688, 'test_runtime': 238.9505, 'test_samples_per_second': 209.248, 'test_steps_per_second': 6.541})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts = trainer.predict(encoded_dataset['test'])\n",
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0c581701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 2), (50000,))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts.predictions.shape, predicts.label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "909213de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predicts.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2d45fc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.90688}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from datasets import load_metric\n",
    "import evaluate\n",
    "\n",
    "#metric = load_metric(\"glue\", \"mrpc\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "metric.compute(predictions=preds, references=predicts.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e291328",
   "metadata": {},
   "source": [
    "* 모델 저장 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5c7f39a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nsmc_model/bert-base-dynamic/tokenizer_config.json',\n",
       " 'nsmc_model/bert-base-dynamic/special_tokens_map.json',\n",
       " 'nsmc_model/bert-base-dynamic/vocab.txt',\n",
       " 'nsmc_model/bert-base-dynamic/added_tokens.json',\n",
       " 'nsmc_model/bert-base-dynamic/tokenizer.json')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_model.save_pretrained('nsmc_model/bert-base-dynamic')\n",
    "huggingface_tokenizer.save_pretrained('nsmc_model/bert-base-dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('nsmc_model/bert-base-dynamic')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('nsmc_model/bert-base-dynamic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432209e2",
   "metadata": {},
   "source": [
    "## STEP 5. 모델 로드 및 테스트 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2e9f2",
   "metadata": {},
   "source": [
    "#### TextClassificationPipeline\n",
    "* 모델의 예측값이 소프트맥스 함수를 통과한 후의 값인 레이블 별 스코어 임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4670061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['걸작은 몇안되고 졸작들만 넘쳐난다.',\n",
       " '이렇게 지겨울수가',\n",
       " 'ost... 정말 좋았어요. 영화 자체도.. 의문은 남지만 ㅎㅎ',\n",
       " '펑퍼짐한 건빵바지(카고)를 입고 오도바이탈때 입는 잠바 같은 것을 입은 주인공겸 감독이 앞차기 옆차기 뒷차기 앞돌려차기등의 기술을 구사해 악당을 소탕하는 영화 주먹질도 하지만 앞차기와 옆차기의 비중이 크다',\n",
       " '솔직히 생각보다 재밌어서 놀랐다... 이거 개봉하고 낸린줄도 모르고...ㅠ ㅠ 이제서야 봤네... 정말 두 사람의 커플댄스는... 명장면이다... 생각할수록 아쉽다... 단순 sm상업영화라고 생각하기엔... 두 사람의 춤이 너무 인상적이다...',\n",
       " '이거 평점보니까 알바고용하는데 제작비보다 많이들어갔겠어요 정말 재미라고는 찾아볼 수 없고 특히 마지막에 여자애가 경상도사람이 들으면 코웃음 칠만큼 어색한 사투리로 엄마한테 말할때는 그나마 잡히던 감정도 깨져버렸습니다']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_nsmc_dataset['test']['document'][100:106]\n",
    "huggingface_nsmc_dataset['test']['label'][100:106]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43174aa",
   "metadata": {},
   "source": [
    "#### Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ac249e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "# 로드하기\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base')\n",
    "\n",
    "text_classifier = TextClassificationPipeline(\n",
    "    tokenizer=loaded_tokenizer, \n",
    "    model=loaded_model, \n",
    "#    framework='tf',\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c1380115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5378297567367554},\n",
       " {'label': 'LABEL_1', 'score': 0.46217024326324463}]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('걸작은 몇안되고 졸작들만 넘쳐난다.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bacc6954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.6520986557006836},\n",
       " {'label': 'LABEL_1', 'score': 0.3479013741016388}]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('ost... 정말 좋았어요. 영화 자체도.. 의문은 남지만 ㅎㅎ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "50e29348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5777747631072998},\n",
       " {'label': 'LABEL_1', 'score': 0.4222252666950226}]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('펑퍼짐한 건빵바지(카고)를 입고 오도바이탈때 입는 잠바 같은 것을 입은 주인공겸 감독이 앞차기 옆차기 뒷차기 앞돌려차기등의 기술을 구사해 악당을 소탕하는 영화 주먹질도 하지만 앞차기와 옆차기의 비중이 크다',)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f3c6570d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5022687315940857},\n",
       " {'label': 'LABEL_1', 'score': 0.4977312684059143}]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('이거 평점보니까 알바고용하는데 제작비보다 많이들어갔겠어요 정말 재미라고는 찾아볼 수 없고 특히 마지막에 여자애가 경상도사람이 들으면 코웃음 칠만큼 어색한 사투리로 엄마한테 말할때는 그나마 잡히던 감정도 깨져버렸습니다')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842390b",
   "metadata": {},
   "source": [
    "#### Fine-tuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d31c4d",
   "metadata": {},
   "source": [
    "#### 1. max_length or longest로 padding 처리 시 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0bec1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "# 로드하기\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained('nsmc_model/bert-base')\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained('nsmc_model/bert-base')\n",
    "\n",
    "text_classifier = TextClassificationPipeline(\n",
    "    tokenizer=loaded_tokenizer, \n",
    "    model=loaded_model, \n",
    "#    framework='tf',\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8f0b1e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9982321858406067},\n",
       " {'label': 'LABEL_1', 'score': 0.0017678668955340981}]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('걸작은 몇안되고 졸작들만 넘쳐난다.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f0832a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.00450678588822484},\n",
       " {'label': 'LABEL_1', 'score': 0.9954931735992432}]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('ost... 정말 좋았어요. 영화 자체도.. 의문은 남지만 ㅎㅎ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "b4273ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.30961325764656067},\n",
       " {'label': 'LABEL_1', 'score': 0.6903867721557617}]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('펑퍼짐한 건빵바지(카고)를 입고 오도바이탈때 입는 잠바 같은 것을 입은 주인공겸 감독이 앞차기 옆차기 뒷차기 앞돌려차기등의 기술을 구사해 악당을 소탕하는 영화 주먹질도 하지만 앞차기와 옆차기의 비중이 크다',)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ee43f8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9991514682769775},\n",
       " {'label': 'LABEL_1', 'score': 0.0008485556463710964}]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('이거 평점보니까 알바고용하는데 제작비보다 많이들어갔겠어요 정말 재미라고는 찾아볼 수 없고 특히 마지막에 여자애가 경상도사람이 들으면 코웃음 칠만큼 어색한 사투리로 엄마한테 말할때는 그나마 잡히던 감정도 깨져버렸습니다')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e518086",
   "metadata": {},
   "source": [
    "#### 2. dynamic padding 처리 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "60793af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "# 로드하기\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained('nsmc_model/bert-base-dynamic')\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained('nsmc_model/bert-base-dynamic')\n",
    "\n",
    "text_classifier = TextClassificationPipeline(\n",
    "    tokenizer=loaded_tokenizer, \n",
    "    model=loaded_model, \n",
    "#    framework='tf',\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b8b29e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9980570673942566},\n",
       " {'label': 'LABEL_1', 'score': 0.0019428539089858532}]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('걸작은 몇안되고 졸작들만 넘쳐난다.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2dd277a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.00419238256290555},\n",
       " {'label': 'LABEL_1', 'score': 0.9958076477050781}]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('ost... 정말 좋았어요. 영화 자체도.. 의문은 남지만 ㅎㅎ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c06d8a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.515796422958374},\n",
       " {'label': 'LABEL_1', 'score': 0.484203577041626}]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('펑퍼짐한 건빵바지(카고)를 입고 오도바이탈때 입는 잠바 같은 것을 입은 주인공겸 감독이 앞차기 옆차기 뒷차기 앞돌려차기등의 기술을 구사해 악당을 소탕하는 영화 주먹질도 하지만 앞차기와 옆차기의 비중이 크다',)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "71a6995e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9989676475524902},\n",
       " {'label': 'LABEL_1', 'score': 0.0010323687456548214}]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier('이거 평점보니까 알바고용하는데 제작비보다 많이들어갔겠어요 정말 재미라고는 찾아볼 수 없고 특히 마지막에 여자애가 경상도사람이 들으면 코웃음 칠만큼 어색한 사투리로 엄마한테 말할때는 그나마 잡히던 감정도 깨져버렸습니다')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179815e",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597d6b5",
   "metadata": {},
   "source": [
    "* Fine-tuning 결과\n",
    "  * 1 epoch 만으로도 val acc 90% 이상 도달 함\n",
    "  * 2 epoch 까지 돌려봤을 때, val loss는 서서히 감소하고, val acc는 서서히 증가하는 추세이므로 더 많은 epoch를 돌린다면, 더 좋은 성능을 보일 것으로 보임\n",
    "  * dynamic padding 수행 시에, mixed length padding 수행시 보다 train 수행 시간은 반으로 줄여들지만, 모델의 성능에는 차이가 없어 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df16cc8",
   "metadata": {},
   "source": [
    "|padding 처리|epoch|Train 수행 시간|val loss|val accuracy|\n",
    "|:-------|:-------|:---------------|:-----|:---|\n",
    "|pixed length padding|2|3시간 12분|0.248412|0.907060|\n",
    "|dynamic padding(group_by_length=False)|2|2시간 22분|0.245954|0.907500|\n",
    "|dynamic padding(group_by_length=True)|2|1시간 43분|0.246578|0.907100|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7aef9",
   "metadata": {},
   "source": [
    "## 참고 문헌\n",
    "* [Transformers (신경망 언어모델 라이브러리 강좌)](https://wikidocs.net/166802)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
